
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <title>4.2-序列标注 · HonKit</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="HonKit 5.1.4">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/@honkit/honkit-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, user-scalable=yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="4.3-问答任务-抽取式问答.html" />
    
    
    <link rel="prev" href="4.1-文本分类.html" />
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">篇章1-前言</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../篇章1-前言/1.0-本地阅读和代码运行环境配置.html">
            
                <a href="../篇章1-前言/1.0-本地阅读和代码运行环境配置.html">
            
                    
                    1.0-本地阅读和代码运行环境配置
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../篇章1-前言/1.1-Transformers在NLP中的兴起.html">
            
                <a href="../篇章1-前言/1.1-Transformers在NLP中的兴起.html">
            
                    
                    1.1-Transformers在NLP中的兴起
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">篇章2-Transformer相关原理</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../篇章2-Transformer相关原理/2.0-前言.html">
            
                <a href="../篇章2-Transformer相关原理/2.0-前言.html">
            
                    
                    2.0-前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../篇章2-Transformer相关原理/2.1-图解attention.html">
            
                <a href="../篇章2-Transformer相关原理/2.1-图解attention.html">
            
                    
                    2.1-图解attention
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../篇章2-Transformer相关原理/2.2-图解transformer.html">
            
                <a href="../篇章2-Transformer相关原理/2.2-图解transformer.html">
            
                    
                    2.2-图解transformer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="../篇章2-Transformer相关原理/2.2.1-Pytorch编写Transformer.html">
            
                <a href="../篇章2-Transformer相关原理/2.2.1-Pytorch编写Transformer.html">
            
                    
                    2.2.1-Pytorch编写Transformer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.5" data-path="../篇章2-Transformer相关原理/2.2.1-Pytorch编写Transformer-选读.md">
            
                <span>
            
                    
                    2.2.2-Pytorch编写Transformer-选读
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.6" data-path="../篇章2-Transformer相关原理/2.3-图解BERT.html">
            
                <a href="../篇章2-Transformer相关原理/2.3-图解BERT.html">
            
                    
                    2.3-图解BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.7" data-path="../篇章2-Transformer相关原理/2.4-图解GPT.html">
            
                <a href="../篇章2-Transformer相关原理/2.4-图解GPT.html">
            
                    
                    2.4-图解GPT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.8" data-path="../篇章2-Transformer相关原理/2.5-篇章小测.html">
            
                <a href="../篇章2-Transformer相关原理/2.5-篇章小测.html">
            
                    
                    2.5-篇章小测
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">篇章3-编写一个Transformer模型：BERT</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../篇章3-编写一个Transformer模型：BERT/3.1-如何实现一个BERT.html">
            
                <a href="../篇章3-编写一个Transformer模型：BERT/3.1-如何实现一个BERT.html">
            
                    
                    3.1-如何实现一个BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="../篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT.html">
            
                <a href="../篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT.html">
            
                    
                    3.2-如何应用一个BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="../篇章3-编写一个Transformer模型：BERT/3.3-篇章小测.html">
            
                <a href="../篇章3-编写一个Transformer模型：BERT/3.3-篇章小测.html">
            
                    
                    3.3-篇章小测
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">篇章4-使用Transformers解决NLP任务</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="4.0-前言.html">
            
                <a href="4.0-前言.html">
            
                    
                    4.0-前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="4.0-基于HuggingFace-Transformers的预训练模型微调.html">
            
                <a href="4.0-基于HuggingFace-Transformers的预训练模型微调.html">
            
                    
                    4.0-基于HuggingFace-Transformers的预训练模型微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="4.1-文本分类.html">
            
                <a href="4.1-文本分类.html">
            
                    
                    4.1-文本分类
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="4.4" data-path="4.2-序列标注.html">
            
                <a href="4.2-序列标注.html">
            
                    
                    4.2-序列标注
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="4.3-问答任务-抽取式问答.html">
            
                <a href="4.3-问答任务-抽取式问答.html">
            
                    
                    4.3-问答任务-抽取式问答
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.6" data-path="4.4-问答任务-多选问答.html">
            
                <a href="4.4-问答任务-多选问答.html">
            
                    
                    4.4-问答任务-多选问答
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.7" data-path="4.5-生成任务-语言模型.html">
            
                <a href="4.5-生成任务-语言模型.html">
            
                    
                    4.5-生成任务-语言模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.8" data-path="4.6-生成任务-机器翻译.html">
            
                <a href="4.6-生成任务-机器翻译.html">
            
                    
                    4.6-生成任务-机器翻译
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.9" data-path="4.7-生成任务-摘要生成.html">
            
                <a href="4.7-生成任务-摘要生成.html">
            
                    
                    4.7-生成任务-摘要生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10" data-path="4.8-篇章小测.html">
            
                <a href="4.8-篇章小测.html">
            
                    
                    4.8-篇章小测
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://github.com/honkit/honkit" target="blank" class="gitbook-link">
            Published with HonKit
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >4.2-序列标注</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <p>本文涉及的jupter notebook在<a href="https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1" target="_blank">篇章4代码库中</a>。</p>
<p>如果您正在google的colab中打开这个notebook，您可能需要安装Transformers和🤗Datasets库。将以下命令取消注释即可安装。</p>
<pre><code class="lang-python">!pip install datasets transformers seqeval
</code></pre>
<p>如果您正在本地打开这个notebook，请确保您已经进行上述依赖包的安装。您也可以在<a href="https://github.com/huggingface/transformers/tree/master/examples/token-classification" target="_blank">这里</a>找到本notebook的多GPU分布式训练版本。</p>
<p>本小节所涉及的模型结构与上一篇章中的BERT基本一致，额外需要学习的是特定任务的数据处理方法和模型训练方法。</p>
<h1 id="序列标注（token级的分类问题）"><em>序列标注（token级的分类问题）</em></h1>
<p>序列标注，通常也可以看作是token级别的分类问题：对每一个token进行分类。在这个notebook中，我们将展示如何使用<a href="https://github.com/huggingface/transformers" target="_blank">🤗 Transformers</a>中的transformer模型去做token级别的分类问题。token级别的分类任务通常指的是为为文本中的每一个token预测一个标签结果。下图展示的是一个NER实体名词识别任务。</p>
<p><img src="https://github.com/huggingface/notebooks/blob/master/examples/images/token_classification.png?raw=1" alt="Widget inference representing the NER task"></img></p>
<p>最常见的token级别分类任务:</p>
<ul>
<li>NER (Named-entity recognition 名词-实体识别) 分辨出文本中的名词和实体 (person人名, organization组织机构名, location地点名...).</li>
<li>POS (Part-of-speech tagging词性标注) 根据语法对token进行词性标注 (noun名词, verb动词, adjective形容词...)</li>
<li>Chunk (Chunking短语组块) 将同一个短语的tokens组块放在一起。</li>
</ul>
<p>对于以上任务，我们将展示如何使用简单的Dataset库加载数据集，同时使用transformer中的<code>Trainer</code>接口对预训练模型进行微调。</p>
<p>只要预训练的transformer模型最顶层有一个token分类的神经网络层（比如上一篇章提到的<code>BertForTokenClassification</code>）（另外，由于transformer库的tokenizer新特性，可能还需要对应的预训练模型有fast tokenizer这个功能，参考<a href="https://huggingface.co/transformers/index.html#bigtable" target="_blank">这个表</a>），那么本notebook理论上可以使用各种各样的transformer模型（<a href="https://huggingface.co/models" target="_blank">模型面板</a>），解决任何token级别的分类任务。</p>
<p>如果您所处理的任务有所不同，大概率只需要很小的改动便可以使用本notebook进行处理。同时，您应该根据您的GPU显存来调整微调训练所需要的btach size大小，避免显存溢出。</p>
<pre><code class="lang-python">task = <span class="hljs-string">"ner"</span> <span class="hljs-comment">#需要是"ner", "pos" 或者 "chunk"</span>
model_checkpoint = <span class="hljs-string">"distilbert-base-uncased"</span>
batch_size = <span class="hljs-number">16</span>
</code></pre>
<h2 id="加载数据">加载数据</h2>
<p>我们将会使用<a href="https://github.com/huggingface/datasets" target="_blank">🤗 Datasets</a>库来加载数据和对应的评测方式。数据加载和评测方式加载只需要简单使用<code>load_dataset</code>和<code>load_metric</code>即可。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
</code></pre>
<p>本notebook中的例子使用的是<a href="https://www.aclweb.org/anthology/W03-0419.pdf" target="_blank">CONLL 2003 dataset</a>数据集。这个notebook应该可以处理🤗 Datasets库中的任何token分类任务。如果您使用的是您自定义的json/csv文件数据集，您需要查看<a href="https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files" target="_blank">数据集文档</a>来学习如何加载。自定义数据集可能需要在加载属性名字上做一些调整。</p>
<pre><code class="lang-python">datasets = load_dataset(<span class="hljs-string">"conll2003"</span>)
</code></pre>
<p>这个<code>datasets</code>对象本身是一种<a href="https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict" target="_blank"><code>DatasetDict</code></a>数据结构. 对于训练集、验证集和测试集，只需要使用对应的key（train，validation，test）即可得到相应的数据。</p>
<pre><code class="lang-python">datasets
</code></pre>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3453
    })
})
</code></pre><p>无论是在训练集、验证机还是测试集中，datasets都包含了一个名为tokens的列（一般来说是将文本切分成了很多词），还包含一个名为label的列，这一列对应这tokens的标注。</p>
<p>给定一个数据切分的key（train、validation或者test）和下标即可查看数据。</p>
<pre><code class="lang-python">datasets[<span class="hljs-string">"train"</span>][<span class="hljs-number">0</span>]
</code></pre>
<pre><code>{'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],
 'id': '0',
 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],
 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],
 'tokens': ['EU',
  'rejects',
  'German',
  'call',
  'to',
  'boycott',
  'British',
  'lamb',
  '.']}
</code></pre><p>所有的数据标签labels都已经被编码成了整数，可以直接被预训练transformer模型使用。这些整数的编码所对应的实际类别储存在<code>features</code>中。</p>
<pre><code class="lang-python">datasets[<span class="hljs-string">"train"</span>].features[<span class="hljs-string">f"ner_tags"</span>]
</code></pre>
<pre><code>Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
</code></pre><p>所以以NER为例，0对应的标签类别是”O“， 1对应的是”B-PER“等等。”O“的意思是没有特别实体（no special entity）。本例包含4种实体类别分别是（PER、ORG、LOC，MISC），每一种实体类别又分别有B-（实体开始的token）前缀和I-（实体中间的token）前缀。</p>
<ul>
<li>'PER' for person</li>
<li>'ORG' for organization</li>
<li>'LOC' for location</li>
<li>'MISC' for miscellaneous</li>
</ul>
<pre><code class="lang-python">label_list = datasets[<span class="hljs-string">"train"</span>].features[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>].feature.names
label_list
</code></pre>
<pre><code>['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
</code></pre><p>为了能够进一步理解数据长什么样子，下面的函数将从数据集里随机选择几个例子进行展示。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel, <span class="hljs-type">Sequence</span>
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> display, HTML

<span class="hljs-keyword">def</span> <span class="hljs-title function_">show_random_elements</span>(<span class="hljs-params">dataset, num_examples=<span class="hljs-number">10</span></span>):
    <span class="hljs-keyword">assert</span> num_examples &lt;= <span class="hljs-built_in">len</span>(dataset), <span class="hljs-string">"Can't pick more elements than there are in the dataset."</span>
    picks = []
    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_examples):
        pick = random.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset)-<span class="hljs-number">1</span>)
        <span class="hljs-keyword">while</span> pick <span class="hljs-keyword">in</span> picks:
            pick = random.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset)-<span class="hljs-number">1</span>)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    <span class="hljs-keyword">for</span> column, typ <span class="hljs-keyword">in</span> dataset.features.items():
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(typ, ClassLabel):
            df[column] = df[column].transform(<span class="hljs-keyword">lambda</span> i: typ.names[i])
        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(typ, <span class="hljs-type">Sequence</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">isinstance</span>(typ.feature, ClassLabel):
            df[column] = df[column].transform(<span class="hljs-keyword">lambda</span> x: [typ.feature.names[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x])
    display(HTML(df.to_html()))
</code></pre>
<pre><code class="lang-python">show_random_elements(datasets[<span class="hljs-string">"train"</span>])
</code></pre>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>tokens</th>
      <th>pos_tags</th>
      <th>chunk_tags</th>
      <th>ner_tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2227</td>
      <td>[Result, of, a, French, first, division, match, on, Friday, .]</td>
      <td>[NN, IN, DT, JJ, JJ, NN, NN, IN, NNP, .]</td>
      <td>[B-NP, B-PP, B-NP, I-NP, I-NP, I-NP, I-NP, B-PP, B-NP, O]</td>
      <td>[O, O, O, B-MISC, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2615</td>
      <td>[Mid-tier, golds, up, in, heavy, trading, .]</td>
      <td>[NN, NNS, IN, IN, JJ, NN, .]</td>
      <td>[B-NP, I-NP, B-PP, B-PP, B-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10256</td>
      <td>[Neagle, (, 14-6, ), beat, the, Braves, for, the, third, time, this, season, ,, allowing, two, runs, and, six, hits, in, eight, innings, .]</td>
      <td>[NNP, (, CD, ), VB, DT, NNPS, IN, DT, JJ, NN, DT, NN, ,, VBG, CD, NNS, CC, CD, NNS, IN, CD, NN, .]</td>
      <td>[B-NP, O, B-NP, O, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, I-NP, B-NP, I-NP, O, B-VP, B-NP, I-NP, O, B-NP, I-NP, B-PP, B-NP, I-NP, O]</td>
      <td>[B-PER, O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10720</td>
      <td>[Hansa, Rostock, 4, 1, 2, 1, 5, 4, 5]</td>
      <td>[NNP, NNP, CD, CD, CD, CD, CD, CD, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7125</td>
      <td>[MONTREAL, 70, 59, .543, 11]</td>
      <td>[NNP, CD, CD, CD, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, O, O, O, O]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3316</td>
      <td>[Softbank, Corp, said, on, Friday, that, it, would, procure, $, 900, million, through, the, foreign, exchange, market, by, September, 5, as, part, of, its, acquisition, of, U.S., firm, ,, Kingston, Technology, Co, .]</td>
      <td>[NNP, NNP, VBD, IN, NNP, IN, PRP, MD, NN, $, CD, CD, IN, DT, JJ, NN, NN, IN, NNP, CD, IN, NN, IN, PRP$, NN, IN, NNP, NN, ,, NNP, NNP, NNP, .]</td>
      <td>[B-NP, I-NP, B-VP, B-PP, B-NP, B-SBAR, B-NP, B-VP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, O, B-NP, I-NP, I-NP, O]</td>
      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, B-ORG, I-ORG, I-ORG, O]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3923</td>
      <td>[Ghent, 3, Aalst, 2]</td>
      <td>[NN, CD, NNP, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, O, B-ORG, O]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2776</td>
      <td>[The, separatists, ,, who, swept, into, Grozny, on, August, 6, ,, still, control, large, areas, of, the, centre, of, town, ,, and, Russian, soldiers, are, based, at, checkpoints, on, the, approach, roads, .]</td>
      <td>[DT, NNS, ,, WP, VBD, IN, NNP, IN, NNP, CD, ,, RB, VBP, JJ, NNS, IN, DT, NN, IN, NN, ,, CC, JJ, NNS, VBP, VBN, IN, NNS, IN, DT, NN, NNS, .]</td>
      <td>[B-NP, I-NP, O, B-NP, B-VP, B-PP, B-NP, B-PP, B-NP, I-NP, O, B-ADVP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, O, O, B-NP, I-NP, B-VP, I-VP, B-PP, B-NP, B-PP, B-NP, I-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1178</td>
      <td>[Doctor, Masserigne, Ndiaye, said, medical, staff, were, overwhelmed, with, work, ., "]</td>
      <td>[NNP, NNP, NNP, VBD, JJ, NN, VBD, VBN, IN, NN, ., "]</td>
      <td>[B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-VP, I-VP, B-PP, B-NP, O, O]</td>
      <td>[O, B-PER, I-PER, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10988</td>
      <td>[Reuters, historical, calendar, -, September, 4, .]</td>
      <td>[NNP, JJ, NN, :, NNP, CD, .]</td>
      <td>[B-NP, I-NP, I-NP, O, B-NP, I-NP, O]</td>
      <td>[B-ORG, O, O, O, O, O, O]</td>
    </tr>
  </tbody>
</table>


<h2 id="预处理数据">预处理数据</h2>
<p>在将数据喂入模型之前，我们需要对数据进行预处理。预处理的工具叫<code>Tokenizer</code>。<code>Tokenizer</code>首先对输入进行tokenize，然后将tokens转化为预模型中需要对应的token ID，再转化为模型需要的输入格式。</p>
<p>为了达到数据预处理的目的，我们使用<code>AutoTokenizer.from_pretrained</code>方法实例化我们的tokenizer，这样可以确保：</p>
<ul>
<li>我们得到一个与预训练模型一一对应的tokenizer。</li>
<li>使用指定的模型checkpoint对应的tokenizer的时候，我们也下载了模型需要的词表库vocabulary，准确来说是tokens vocabulary。</li>
</ul>
<p>这个被下载的tokens vocabulary会被缓存起来，从而再次使用的时候不会重新下载。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
</code></pre>
<p>注意：以下代码要求tokenizer必须是transformers.PreTrainedTokenizerFast类型，因为我们在预处理的时候需要用到fast tokenizer的一些特殊特性（比如多线程快速tokenizer）。</p>
<p>几乎所有模型对应的tokenizer都有对应的fast tokenizer。我们可以在<a href="https://huggingface.co/transformers/index.html#bigtable" target="_blank">模型tokenizer对应表</a>里查看所有预训练模型对应的tokenizer所拥有的特点。</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> transformers
<span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(tokenizer, transformers.PreTrainedTokenizerFast)
</code></pre>
<p>在<a href="https://huggingface.co/transformers/index.html#bigtable" target="_blank">这里big table of models</a>查看模型是否有fast tokenizer。</p>
<p>tokenizer既可以对单个文本进行预处理，也可以对一对文本进行预处理，tokenizer预处理后得到的数据满足预训练模型输入格式</p>
<pre><code class="lang-python">tokenizer(<span class="hljs-string">"Hello, this is one sentence!"</span>)
</code></pre>
<pre><code>{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre><pre><code class="lang-python">tokenizer([<span class="hljs-string">"Hello"</span>, <span class="hljs-string">","</span>, <span class="hljs-string">"this"</span>, <span class="hljs-string">"is"</span>, <span class="hljs-string">"one"</span>, <span class="hljs-string">"sentence"</span>, <span class="hljs-string">"split"</span>, <span class="hljs-string">"into"</span>, <span class="hljs-string">"words"</span>, <span class="hljs-string">"."</span>], is_split_into_words=<span class="hljs-literal">True</span>)
</code></pre>
<pre><code>{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 3975, 2046, 2616, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre><p>注意transformer预训练模型在预训练的时候通常使用的是subword，如果我们的文本输入已经被切分成了word，那么这些word还会被我们的tokenizer继续切分。举个例子：</p>
<pre><code class="lang-python">example = datasets[<span class="hljs-string">"train"</span>][<span class="hljs-number">4</span>]
<span class="hljs-built_in">print</span>(example[<span class="hljs-string">"tokens"</span>])
</code></pre>
<pre><code>['Germany', "'s", 'representative', 'to', 'the', 'European', 'Union', "'s", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']
</code></pre><pre><code class="lang-python">tokenized_input = tokenizer(example[<span class="hljs-string">"tokens"</span>], is_split_into_words=<span class="hljs-literal">True</span>)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input[<span class="hljs-string">"input_ids"</span>])
<span class="hljs-built_in">print</span>(tokens)
</code></pre>
<pre><code>['[CLS]', 'germany', "'", 's', 'representative', 'to', 'the', 'european', 'union', "'", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']
</code></pre><p>单词"Zwingmann" 和 "sheepmeat"继续被切分成了3个subtokens。</p>
<p>由于标注数据通常是在word级别进行标注的，既然word还会被切分成subtokens，那么意味着我们还需要对标注数据进行subtokens的对齐。同时，由于预训练模型输入格式的要求，往往还需要加上一些特殊符号比如： <code>[CLS]</code> 和  <code>[SEP]</code>。</p>
<pre><code class="lang-python"><span class="hljs-built_in">len</span>(example[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>]), <span class="hljs-built_in">len</span>(tokenized_input[<span class="hljs-string">"input_ids"</span>])
</code></pre>
<pre><code>(31, 39)
</code></pre><p>tokenizer有一个<code>`word_ids</code>方法可以帮助我们解决这个问题。</p>
<pre><code class="lang-python"><span class="hljs-built_in">print</span>(tokenized_input.word_ids())
</code></pre>
<pre><code>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]
</code></pre><p>我们可以看到，word_ids将每一个subtokens位置都对应了一个word的下标。比如第1个位置对应第0个word，然后第2、3个位置对应第1个word。特殊字符对应了None。有了这个list，我们就能将subtokens和words还有标注的labels对齐啦。</p>
<pre><code class="lang-python">word_ids = tokenized_input.word_ids()
aligned_labels = [-<span class="hljs-number">100</span> <span class="hljs-keyword">if</span> i <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> example[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> word_ids]
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(aligned_labels), <span class="hljs-built_in">len</span>(tokenized_input[<span class="hljs-string">"input_ids"</span>]))
</code></pre>
<pre><code>39 39
</code></pre><p>我们通常将特殊字符的label设置为-100，在模型中-100通常会被忽略掉不计算loss。</p>
<p>我们有两种对齐label的方式：</p>
<ul>
<li>多个subtokens对齐一个word，对齐一个label</li>
<li>多个subtokens的第一个subtoken对齐word，对齐一个label，其他subtokens直接赋予-100.</li>
</ul>
<p>我们提供这两种方式，通过<code>label_all_tokens = True</code>切换。</p>
<pre><code class="lang-python">label_all_tokens = <span class="hljs-literal">True</span>
</code></pre>
<p>最后我们将所有内容合起来变成我们的预处理函数。<code>is_split_into_words=True</code>在上面已经结束啦。</p>
<pre><code class="lang-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_and_align_labels</span>(<span class="hljs-params">examples</span>):
    tokenized_inputs = tokenizer(examples[<span class="hljs-string">"tokens"</span>], truncation=<span class="hljs-literal">True</span>, is_split_into_words=<span class="hljs-literal">True</span>)

    labels = []
    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(examples[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = <span class="hljs-literal">None</span>
        label_ids = []
        <span class="hljs-keyword">for</span> word_idx <span class="hljs-keyword">in</span> word_ids:
            <span class="hljs-comment"># Special tokens have a word id that is None. We set the label to -100 so they are automatically</span>
            <span class="hljs-comment"># ignored in the loss function.</span>
            <span class="hljs-keyword">if</span> word_idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
                label_ids.append(-<span class="hljs-number">100</span>)
            <span class="hljs-comment"># We set the label for the first token of each word.</span>
            <span class="hljs-keyword">elif</span> word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            <span class="hljs-comment"># For the other tokens in a word, we set the label to either the current label or -100, depending on</span>
            <span class="hljs-comment"># the label_all_tokens flag.</span>
            <span class="hljs-keyword">else</span>:
                label_ids.append(label[word_idx] <span class="hljs-keyword">if</span> label_all_tokens <span class="hljs-keyword">else</span> -<span class="hljs-number">100</span>)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs[<span class="hljs-string">"labels"</span>] = labels
    <span class="hljs-keyword">return</span> tokenized_inputs
</code></pre>
<p>以上的预处理函数可以处理一个样本，也可以处理多个样本exapmles。如果是处理多个样本，则返回的是多个样本被预处理之后的结果list。</p>
<pre><code class="lang-python">tokenize_and_align_labels(datasets[<span class="hljs-string">'train'</span>][:<span class="hljs-number">5</span>])
</code></pre>
<pre><code>{'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}
</code></pre><p>接下来对数据集datasets里面的所有样本进行预处理，处理的方式是使用map函数，将预处理函数prepare_train_features应用到（map)所有样本上。</p>
<pre><code class="lang-python">tokenized_datasets = datasets.<span class="hljs-built_in">map</span>(tokenize_and_align_labels, batched=<span class="hljs-literal">True</span>)
</code></pre>
<p>更好的是，返回的结果会自动被缓存，避免下次处理的时候重新计算（但是也要注意，如果输入有改动，可能会被缓存影响！）。datasets库函数会对输入的参数进行检测，判断是否有变化，如果没有变化就使用缓存数据，如果有变化就重新处理。但如果输入参数不变，想改变输入的时候，最好清理调这个缓存。清理的方式是使用<code>load_from_cache_file=False</code>参数。另外，上面使用到的<code>batched=True</code>这个参数是tokenizer的特点，因为这会使用多线程同时并行对输入进行处理。</p>
<h2 id="微调预训练模型">微调预训练模型</h2>
<p>既然数据已经准备好了，现在我们需要下载并加载我们的预训练模型，然后微调预训练模型。既然我们是做seq2seq任务，那么我们需要一个能解决这个任务的模型类。我们使用<code>AutoModelForTokenClassification</code> 这个类。和tokenizer相似，<code>from_pretrained</code>方法同样可以帮助我们下载并加载模型，同时也会对模型进行缓存，就不会重复下载模型啦。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=<span class="hljs-built_in">len</span>(label_list))
</code></pre>
<pre><code>Downloading:   0%|          | 0.00/268M [00:00&lt;?, ?B/s]


Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre><p>由于我们微调的任务是token分类任务，而我们加载的是预训练的语言模型，所以会提示我们加载模型的时候扔掉了一些不匹配的神经网络参数（比如：预训练语言模型的神经网络head被扔掉了，同时随机初始化了token分类的神经网络head）。</p>
<p>为了能够得到一个<code>Trainer</code>训练工具，我们还需要3个要素，其中最重要的是训练的设定/参数 <a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments" target="_blank"><code>TrainingArguments</code></a>。这个训练设定包含了能够定义训练过程的所有属性。</p>
<pre><code class="lang-python">args = TrainingArguments(
    <span class="hljs-string">f"test-<span class="hljs-subst">{task}</span>"</span>,
    evaluation_strategy = <span class="hljs-string">"epoch"</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)
</code></pre>
<p>上面evaluation_strategy = "epoch"参数告诉训练代码：我们每个epcoh会做一次验证评估。</p>
<p>上面batch_size在这个notebook之前定义好了。</p>
<p>最后我们需要一个数据收集器data collator，将我们处理好的输入喂给模型。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer)
</code></pre>
<p>设置好<code>Trainer</code>还剩最后一件事情，那就是我们需要定义好评估方法。我们使用<a href="https://github.com/chakki-works/seqeval" target="_blank"><code>seqeval</code></a> metric来完成评估。将模型预测送入评估之前，我们也会做一些数据后处理：</p>
<pre><code class="lang-python">metric = load_metric(<span class="hljs-string">"seqeval"</span>)
</code></pre>
<p>评估的输入是预测和label的list</p>
<pre><code class="lang-python">labels = [label_list[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> example[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>]]
metric.compute(predictions=[labels], references=[labels])
</code></pre>
<pre><code>{'LOC': {'f1': 1.0, 'number': 2, 'precision': 1.0, 'recall': 1.0},
 'ORG': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},
 'PER': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},
 'overall_accuracy': 1.0,
 'overall_f1': 1.0,
 'overall_precision': 1.0,
 'overall_recall': 1.0}
</code></pre><p>对模型预测结果做一些后处理：</p>
<ul>
<li>选择预测分类最大概率的下标</li>
<li>将下标转化为label</li>
<li>忽略-100所在地方</li>
</ul>
<p>下面的函数将上面的步骤合并了起来。</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">p</span>):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=<span class="hljs-number">2</span>)

    <span class="hljs-comment"># Remove ignored index (special tokens)</span>
    true_predictions = [
        [label_list[p] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
        <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
    ]
    true_labels = [
        [label_list[l] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
        <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">"precision"</span>: results[<span class="hljs-string">"overall_precision"</span>],
        <span class="hljs-string">"recall"</span>: results[<span class="hljs-string">"overall_recall"</span>],
        <span class="hljs-string">"f1"</span>: results[<span class="hljs-string">"overall_f1"</span>],
        <span class="hljs-string">"accuracy"</span>: results[<span class="hljs-string">"overall_accuracy"</span>],
    }
</code></pre>
<p>我们计算所有类别总的precision/recall/f1，所以会扔掉单个类别的precision/recall/f1 </p>
<p>将数据/模型/参数传入<code>Trainer</code>即可</p>
<pre><code class="lang-python">trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">"train"</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">"validation"</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
</code></pre>
<p>调用<code>train</code>方法开始训练</p>
<pre><code class="lang-python">trainer.train()
</code></pre>
<pre><code>&lt;div&gt;
    &lt;style&gt;
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
    &lt;/style&gt;

  &lt;progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'&gt;&lt;/progress&gt;
  [2634/2634 01:45, Epoch 3/3]
&lt;/div&gt;
&lt;table border="1" class="dataframe"&gt;
</code></pre><p>  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.237721</td>
      <td>0.068198</td>
      <td>0.903148</td>
      <td>0.921132</td>
      <td>0.912051</td>
      <td>0.979713</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.053160</td>
      <td>0.059337</td>
      <td>0.927697</td>
      <td>0.932990</td>
      <td>0.930336</td>
      <td>0.983113</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.029850</td>
      <td>0.059346</td>
      <td>0.929267</td>
      <td>0.939143</td>
      <td>0.934179</td>
      <td>0.984257</td>
    </tr>
  </tbody>
&lt;/table&gt;</p><p></p>
<pre><code>TrainOutput(global_step=2634, training_loss=0.08569671253227518)
</code></pre><p>我们可以再次使用<code>evaluate</code>方法评估，可以评估其他数据集。</p>
<pre><code class="lang-python">trainer.evaluate()
</code></pre>
<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
    </style>

  <progress value="408" max="204" style="width:300px; height:20px; vertical-align: middle;"></progress>
  [204/204 00:05]
</div>






<pre><code>{'eval_loss': 0.05934586375951767,
 'eval_precision': 0.9292672127518264,
 'eval_recall': 0.9391430808815304,
 'eval_f1': 0.9341790463472988,
 'eval_accuracy': 0.9842565968195466,
 'epoch': 3.0}
</code></pre><p>如果想要得到单个类别的precision/recall/f1，我们直接将结果输入相同的评估函数即可：</p>
<pre><code class="lang-python">predictions, labels, _ = trainer.predict(tokenized_datasets[<span class="hljs-string">"validation"</span>])
predictions = np.argmax(predictions, axis=<span class="hljs-number">2</span>)

<span class="hljs-comment"># Remove ignored index (special tokens)</span>
true_predictions = [
    [label_list[p] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
    <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
]
true_labels = [
    [label_list[l] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
    <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
]

results = metric.compute(predictions=true_predictions, references=true_labels)
results
</code></pre>
<pre><code>{'LOC': {'precision': 0.949718574108818,
  'recall': 0.966768525592055,
  'f1': 0.9581677077418134,
  'number': 2618},
 'MISC': {'precision': 0.8132387706855791,
  'recall': 0.8383428107229894,
  'f1': 0.8255999999999999,
  'number': 1231},
 'ORG': {'precision': 0.9055232558139535,
  'recall': 0.9090466926070039,
  'f1': 0.9072815533980583,
  'number': 2056},
 'PER': {'precision': 0.9759552042160737,
  'recall': 0.9765985497692815,
  'f1': 0.9762767710049424,
  'number': 3034},
 'overall_precision': 0.9292672127518264,
 'overall_recall': 0.9391430808815304,
 'overall_f1': 0.9341790463472988,
 'overall_accuracy': 0.9842565968195466}
</code></pre><p>最后别忘了，上传模型到<a href="https://huggingface.co/models" target="_blank">🤗 Model Hub</a>（点击<a href="https://huggingface.co/transformers/model_sharing.html" target="_blank">这里</a>来查看如何上传）。随后您就可以像这个notebook一开始一样，直接用模型名字就能使用您自己上传的模型啦。</p>
<pre><code class="lang-python">

</code></pre>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="4.1-文本分类.html" class="navigation navigation-prev " aria-label="Previous page: 4.1-文本分类">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="4.3-问答任务-抽取式问答.html" class="navigation navigation-next " aria-label="Next page: 4.3-问答任务-抽取式问答">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"4.2-序列标注","level":"4.4","depth":1,"next":{"title":"4.3-问答任务-抽取式问答","level":"4.5","depth":1,"path":"篇章4-使用Transformers解决NLP任务/4.3-问答任务-抽取式问答.md","ref":"./篇章4-使用Transformers解决NLP任务/4.3-问答任务-抽取式问答.md","articles":[]},"previous":{"title":"4.1-文本分类","level":"4.3","depth":1,"path":"篇章4-使用Transformers解决NLP任务/4.1-文本分类.md","ref":"./篇章4-使用Transformers解决NLP任务/4.1-文本分类.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"篇章4-使用Transformers解决NLP任务/4.2-序列标注.md","mtime":"2024-08-23T15:34:37.387Z","type":"markdown"},"gitbook":{"version":"5.1.4","time":"2024-08-23T15:50:04.957Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

