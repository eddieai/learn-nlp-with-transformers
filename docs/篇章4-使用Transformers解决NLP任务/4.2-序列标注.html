
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <title>4.2-åºåˆ—æ ‡æ³¨ Â· HonKit</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="HonKit 5.1.4">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/@honkit/honkit-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, user-scalable=yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="4.3-é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”.html" />
    
    
    <link rel="prev" href="4.1-æ–‡æœ¬åˆ†ç±».html" />
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">ç¯‡ç« 1-å‰è¨€</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../ç¯‡ç« 1-å‰è¨€/1.0-æœ¬åœ°é˜…è¯»å’Œä»£ç è¿è¡Œç¯å¢ƒé…ç½®.html">
            
                <a href="../ç¯‡ç« 1-å‰è¨€/1.0-æœ¬åœ°é˜…è¯»å’Œä»£ç è¿è¡Œç¯å¢ƒé…ç½®.html">
            
                    
                    1.0-æœ¬åœ°é˜…è¯»å’Œä»£ç è¿è¡Œç¯å¢ƒé…ç½®
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../ç¯‡ç« 1-å‰è¨€/1.1-Transformersåœ¨NLPä¸­çš„å…´èµ·.html">
            
                <a href="../ç¯‡ç« 1-å‰è¨€/1.1-Transformersåœ¨NLPä¸­çš„å…´èµ·.html">
            
                    
                    1.1-Transformersåœ¨NLPä¸­çš„å…´èµ·
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">ç¯‡ç« 2-Transformerç›¸å…³åŸç†</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.0-å‰è¨€.html">
            
                <a href="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.0-å‰è¨€.html">
            
                    
                    2.0-å‰è¨€
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.1-å›¾è§£attention.html">
            
                <a href="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.1-å›¾è§£attention.html">
            
                    
                    2.1-å›¾è§£attention
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.2-å›¾è§£transformer.html">
            
                <a href="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.2-å›¾è§£transformer.html">
            
                    
                    2.2-å›¾è§£transformer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.2.1-Pytorchç¼–å†™Transformer.html">
            
                <a href="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.2.1-Pytorchç¼–å†™Transformer.html">
            
                    
                    2.2.1-Pytorchç¼–å†™Transformer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.5" data-path="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.2.1-Pytorchç¼–å†™Transformer-é€‰è¯».md">
            
                <span>
            
                    
                    2.2.2-Pytorchç¼–å†™Transformer-é€‰è¯»
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.6" data-path="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.3-å›¾è§£BERT.html">
            
                <a href="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.3-å›¾è§£BERT.html">
            
                    
                    2.3-å›¾è§£BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.7" data-path="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.4-å›¾è§£GPT.html">
            
                <a href="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.4-å›¾è§£GPT.html">
            
                    
                    2.4-å›¾è§£GPT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.8" data-path="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.5-ç¯‡ç« å°æµ‹.html">
            
                <a href="../ç¯‡ç« 2-Transformerç›¸å…³åŸç†/2.5-ç¯‡ç« å°æµ‹.html">
            
                    
                    2.5-ç¯‡ç« å°æµ‹
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">ç¯‡ç« 3-ç¼–å†™ä¸€ä¸ªTransformeræ¨¡å‹ï¼šBERT</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../ç¯‡ç« 3-ç¼–å†™ä¸€ä¸ªTransformeræ¨¡å‹ï¼šBERT/3.1-å¦‚ä½•å®ç°ä¸€ä¸ªBERT.html">
            
                <a href="../ç¯‡ç« 3-ç¼–å†™ä¸€ä¸ªTransformeræ¨¡å‹ï¼šBERT/3.1-å¦‚ä½•å®ç°ä¸€ä¸ªBERT.html">
            
                    
                    3.1-å¦‚ä½•å®ç°ä¸€ä¸ªBERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="../ç¯‡ç« 3-ç¼–å†™ä¸€ä¸ªTransformeræ¨¡å‹ï¼šBERT/3.2-å¦‚ä½•åº”ç”¨ä¸€ä¸ªBERT.html">
            
                <a href="../ç¯‡ç« 3-ç¼–å†™ä¸€ä¸ªTransformeræ¨¡å‹ï¼šBERT/3.2-å¦‚ä½•åº”ç”¨ä¸€ä¸ªBERT.html">
            
                    
                    3.2-å¦‚ä½•åº”ç”¨ä¸€ä¸ªBERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="../ç¯‡ç« 3-ç¼–å†™ä¸€ä¸ªTransformeræ¨¡å‹ï¼šBERT/3.3-ç¯‡ç« å°æµ‹.html">
            
                <a href="../ç¯‡ç« 3-ç¼–å†™ä¸€ä¸ªTransformeræ¨¡å‹ï¼šBERT/3.3-ç¯‡ç« å°æµ‹.html">
            
                    
                    3.3-ç¯‡ç« å°æµ‹
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="4.0-å‰è¨€.html">
            
                <a href="4.0-å‰è¨€.html">
            
                    
                    4.0-å‰è¨€
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="4.0-åŸºäºHuggingFace-Transformersçš„é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ.html">
            
                <a href="4.0-åŸºäºHuggingFace-Transformersçš„é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ.html">
            
                    
                    4.0-åŸºäºHuggingFace-Transformersçš„é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="4.1-æ–‡æœ¬åˆ†ç±».html">
            
                <a href="4.1-æ–‡æœ¬åˆ†ç±».html">
            
                    
                    4.1-æ–‡æœ¬åˆ†ç±»
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="4.4" data-path="4.2-åºåˆ—æ ‡æ³¨.html">
            
                <a href="4.2-åºåˆ—æ ‡æ³¨.html">
            
                    
                    4.2-åºåˆ—æ ‡æ³¨
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="4.3-é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”.html">
            
                <a href="4.3-é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”.html">
            
                    
                    4.3-é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.6" data-path="4.4-é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”.html">
            
                <a href="4.4-é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”.html">
            
                    
                    4.4-é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.7" data-path="4.5-ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹.html">
            
                <a href="4.5-ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹.html">
            
                    
                    4.5-ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.8" data-path="4.6-ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘.html">
            
                <a href="4.6-ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘.html">
            
                    
                    4.6-ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.9" data-path="4.7-ç”Ÿæˆä»»åŠ¡-æ‘˜è¦ç”Ÿæˆ.html">
            
                <a href="4.7-ç”Ÿæˆä»»åŠ¡-æ‘˜è¦ç”Ÿæˆ.html">
            
                    
                    4.7-ç”Ÿæˆä»»åŠ¡-æ‘˜è¦ç”Ÿæˆ
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10" data-path="4.8-ç¯‡ç« å°æµ‹.html">
            
                <a href="4.8-ç¯‡ç« å°æµ‹.html">
            
                    
                    4.8-ç¯‡ç« å°æµ‹
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://github.com/honkit/honkit" target="blank" class="gitbook-link">
            Published with HonKit
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >4.2-åºåˆ—æ ‡æ³¨</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <p>æœ¬æ–‡æ¶‰åŠçš„jupter notebookåœ¨<a href="https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1" target="_blank">ç¯‡ç« 4ä»£ç åº“ä¸­</a>ã€‚</p>
<p>å¦‚æœæ‚¨æ­£åœ¨googleçš„colabä¸­æ‰“å¼€è¿™ä¸ªnotebookï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…Transformerså’ŒğŸ¤—Datasetsåº“ã€‚å°†ä»¥ä¸‹å‘½ä»¤å–æ¶ˆæ³¨é‡Šå³å¯å®‰è£…ã€‚</p>
<pre><code class="lang-python">!pip install datasets transformers seqeval
</code></pre>
<p>å¦‚æœæ‚¨æ­£åœ¨æœ¬åœ°æ‰“å¼€è¿™ä¸ªnotebookï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»è¿›è¡Œä¸Šè¿°ä¾èµ–åŒ…çš„å®‰è£…ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨<a href="https://github.com/huggingface/transformers/tree/master/examples/token-classification" target="_blank">è¿™é‡Œ</a>æ‰¾åˆ°æœ¬notebookçš„å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒç‰ˆæœ¬ã€‚</p>
<p>æœ¬å°èŠ‚æ‰€æ¶‰åŠçš„æ¨¡å‹ç»“æ„ä¸ä¸Šä¸€ç¯‡ç« ä¸­çš„BERTåŸºæœ¬ä¸€è‡´ï¼Œé¢å¤–éœ€è¦å­¦ä¹ çš„æ˜¯ç‰¹å®šä»»åŠ¡çš„æ•°æ®å¤„ç†æ–¹æ³•å’Œæ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚</p>
<h1 id="åºåˆ—æ ‡æ³¨ï¼ˆtokençº§çš„åˆ†ç±»é—®é¢˜ï¼‰"><em>åºåˆ—æ ‡æ³¨ï¼ˆtokençº§çš„åˆ†ç±»é—®é¢˜ï¼‰</em></h1>
<p>åºåˆ—æ ‡æ³¨ï¼Œé€šå¸¸ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯tokençº§åˆ«çš„åˆ†ç±»é—®é¢˜ï¼šå¯¹æ¯ä¸€ä¸ªtokenè¿›è¡Œåˆ†ç±»ã€‚åœ¨è¿™ä¸ªnotebookä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨<a href="https://github.com/huggingface/transformers" target="_blank">ğŸ¤— Transformers</a>ä¸­çš„transformeræ¨¡å‹å»åštokençº§åˆ«çš„åˆ†ç±»é—®é¢˜ã€‚tokençº§åˆ«çš„åˆ†ç±»ä»»åŠ¡é€šå¸¸æŒ‡çš„æ˜¯ä¸ºä¸ºæ–‡æœ¬ä¸­çš„æ¯ä¸€ä¸ªtokené¢„æµ‹ä¸€ä¸ªæ ‡ç­¾ç»“æœã€‚ä¸‹å›¾å±•ç¤ºçš„æ˜¯ä¸€ä¸ªNERå®ä½“åè¯è¯†åˆ«ä»»åŠ¡ã€‚</p>
<p><img src="https://github.com/huggingface/notebooks/blob/master/examples/images/token_classification.png?raw=1" alt="Widget inference representing the NER task"></img></p>
<p>æœ€å¸¸è§çš„tokençº§åˆ«åˆ†ç±»ä»»åŠ¡:</p>
<ul>
<li>NER (Named-entity recognition åè¯-å®ä½“è¯†åˆ«) åˆ†è¾¨å‡ºæ–‡æœ¬ä¸­çš„åè¯å’Œå®ä½“ (personäººå, organizationç»„ç»‡æœºæ„å, locationåœ°ç‚¹å...).</li>
<li>POS (Part-of-speech taggingè¯æ€§æ ‡æ³¨) æ ¹æ®è¯­æ³•å¯¹tokenè¿›è¡Œè¯æ€§æ ‡æ³¨ (nounåè¯, verbåŠ¨è¯, adjectiveå½¢å®¹è¯...)</li>
<li>Chunk (ChunkingçŸ­è¯­ç»„å—) å°†åŒä¸€ä¸ªçŸ­è¯­çš„tokensç»„å—æ”¾åœ¨ä¸€èµ·ã€‚</li>
</ul>
<p>å¯¹äºä»¥ä¸Šä»»åŠ¡ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç®€å•çš„Datasetåº“åŠ è½½æ•°æ®é›†ï¼ŒåŒæ—¶ä½¿ç”¨transformerä¸­çš„<code>Trainer</code>æ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</p>
<p>åªè¦é¢„è®­ç»ƒçš„transformeræ¨¡å‹æœ€é¡¶å±‚æœ‰ä¸€ä¸ªtokenåˆ†ç±»çš„ç¥ç»ç½‘ç»œå±‚ï¼ˆæ¯”å¦‚ä¸Šä¸€ç¯‡ç« æåˆ°çš„<code>BertForTokenClassification</code>ï¼‰ï¼ˆå¦å¤–ï¼Œç”±äºtransformeråº“çš„tokenizeræ–°ç‰¹æ€§ï¼Œå¯èƒ½è¿˜éœ€è¦å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹æœ‰fast tokenizerè¿™ä¸ªåŠŸèƒ½ï¼Œå‚è€ƒ<a href="https://huggingface.co/transformers/index.html#bigtable" target="_blank">è¿™ä¸ªè¡¨</a>ï¼‰ï¼Œé‚£ä¹ˆæœ¬notebookç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å„ç§å„æ ·çš„transformeræ¨¡å‹ï¼ˆ<a href="https://huggingface.co/models" target="_blank">æ¨¡å‹é¢æ¿</a>ï¼‰ï¼Œè§£å†³ä»»ä½•tokençº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ã€‚</p>
<p>å¦‚æœæ‚¨æ‰€å¤„ç†çš„ä»»åŠ¡æœ‰æ‰€ä¸åŒï¼Œå¤§æ¦‚ç‡åªéœ€è¦å¾ˆå°çš„æ”¹åŠ¨ä¾¿å¯ä»¥ä½¿ç”¨æœ¬notebookè¿›è¡Œå¤„ç†ã€‚åŒæ—¶ï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‚¨çš„GPUæ˜¾å­˜æ¥è°ƒæ•´å¾®è°ƒè®­ç»ƒæ‰€éœ€è¦çš„btach sizeå¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºã€‚</p>
<pre><code class="lang-python">task = <span class="hljs-string">"ner"</span> <span class="hljs-comment">#éœ€è¦æ˜¯"ner", "pos" æˆ–è€… "chunk"</span>
model_checkpoint = <span class="hljs-string">"distilbert-base-uncased"</span>
batch_size = <span class="hljs-number">16</span>
</code></pre>
<h2 id="åŠ è½½æ•°æ®">åŠ è½½æ•°æ®</h2>
<p>æˆ‘ä»¬å°†ä¼šä½¿ç”¨<a href="https://github.com/huggingface/datasets" target="_blank">ğŸ¤— Datasets</a>åº“æ¥åŠ è½½æ•°æ®å’Œå¯¹åº”çš„è¯„æµ‹æ–¹å¼ã€‚æ•°æ®åŠ è½½å’Œè¯„æµ‹æ–¹å¼åŠ è½½åªéœ€è¦ç®€å•ä½¿ç”¨<code>load_dataset</code>å’Œ<code>load_metric</code>å³å¯ã€‚</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
</code></pre>
<p>æœ¬notebookä¸­çš„ä¾‹å­ä½¿ç”¨çš„æ˜¯<a href="https://www.aclweb.org/anthology/W03-0419.pdf" target="_blank">CONLL 2003 dataset</a>æ•°æ®é›†ã€‚è¿™ä¸ªnotebookåº”è¯¥å¯ä»¥å¤„ç†ğŸ¤— Datasetsåº“ä¸­çš„ä»»ä½•tokenåˆ†ç±»ä»»åŠ¡ã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯æ‚¨è‡ªå®šä¹‰çš„json/csvæ–‡ä»¶æ•°æ®é›†ï¼Œæ‚¨éœ€è¦æŸ¥çœ‹<a href="https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files" target="_blank">æ•°æ®é›†æ–‡æ¡£</a>æ¥å­¦ä¹ å¦‚ä½•åŠ è½½ã€‚è‡ªå®šä¹‰æ•°æ®é›†å¯èƒ½éœ€è¦åœ¨åŠ è½½å±æ€§åå­—ä¸Šåšä¸€äº›è°ƒæ•´ã€‚</p>
<pre><code class="lang-python">datasets = load_dataset(<span class="hljs-string">"conll2003"</span>)
</code></pre>
<p>è¿™ä¸ª<code>datasets</code>å¯¹è±¡æœ¬èº«æ˜¯ä¸€ç§<a href="https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict" target="_blank"><code>DatasetDict</code></a>æ•°æ®ç»“æ„. å¯¹äºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œåªéœ€è¦ä½¿ç”¨å¯¹åº”çš„keyï¼ˆtrainï¼Œvalidationï¼Œtestï¼‰å³å¯å¾—åˆ°ç›¸åº”çš„æ•°æ®ã€‚</p>
<pre><code class="lang-python">datasets
</code></pre>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3453
    })
})
</code></pre><p>æ— è®ºæ˜¯åœ¨è®­ç»ƒé›†ã€éªŒè¯æœºè¿˜æ˜¯æµ‹è¯•é›†ä¸­ï¼Œdatasetséƒ½åŒ…å«äº†ä¸€ä¸ªåä¸ºtokensçš„åˆ—ï¼ˆä¸€èˆ¬æ¥è¯´æ˜¯å°†æ–‡æœ¬åˆ‡åˆ†æˆäº†å¾ˆå¤šè¯ï¼‰ï¼Œè¿˜åŒ…å«ä¸€ä¸ªåä¸ºlabelçš„åˆ—ï¼Œè¿™ä¸€åˆ—å¯¹åº”è¿™tokensçš„æ ‡æ³¨ã€‚</p>
<p>ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚</p>
<pre><code class="lang-python">datasets[<span class="hljs-string">"train"</span>][<span class="hljs-number">0</span>]
</code></pre>
<pre><code>{'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],
 'id': '0',
 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],
 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],
 'tokens': ['EU',
  'rejects',
  'German',
  'call',
  'to',
  'boycott',
  'British',
  'lamb',
  '.']}
</code></pre><p>æ‰€æœ‰çš„æ•°æ®æ ‡ç­¾labelséƒ½å·²ç»è¢«ç¼–ç æˆäº†æ•´æ•°ï¼Œå¯ä»¥ç›´æ¥è¢«é¢„è®­ç»ƒtransformeræ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›æ•´æ•°çš„ç¼–ç æ‰€å¯¹åº”çš„å®é™…ç±»åˆ«å‚¨å­˜åœ¨<code>features</code>ä¸­ã€‚</p>
<pre><code class="lang-python">datasets[<span class="hljs-string">"train"</span>].features[<span class="hljs-string">f"ner_tags"</span>]
</code></pre>
<pre><code>Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
</code></pre><p>æ‰€ä»¥ä»¥NERä¸ºä¾‹ï¼Œ0å¯¹åº”çš„æ ‡ç­¾ç±»åˆ«æ˜¯â€Oâ€œï¼Œ 1å¯¹åº”çš„æ˜¯â€B-PERâ€œç­‰ç­‰ã€‚â€Oâ€œçš„æ„æ€æ˜¯æ²¡æœ‰ç‰¹åˆ«å®ä½“ï¼ˆno special entityï¼‰ã€‚æœ¬ä¾‹åŒ…å«4ç§å®ä½“ç±»åˆ«åˆ†åˆ«æ˜¯ï¼ˆPERã€ORGã€LOCï¼ŒMISCï¼‰ï¼Œæ¯ä¸€ç§å®ä½“ç±»åˆ«åˆåˆ†åˆ«æœ‰B-ï¼ˆå®ä½“å¼€å§‹çš„tokenï¼‰å‰ç¼€å’ŒI-ï¼ˆå®ä½“ä¸­é—´çš„tokenï¼‰å‰ç¼€ã€‚</p>
<ul>
<li>'PER' for person</li>
<li>'ORG' for organization</li>
<li>'LOC' for location</li>
<li>'MISC' for miscellaneous</li>
</ul>
<pre><code class="lang-python">label_list = datasets[<span class="hljs-string">"train"</span>].features[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>].feature.names
label_list
</code></pre>
<pre><code>['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
</code></pre><p>ä¸ºäº†èƒ½å¤Ÿè¿›ä¸€æ­¥ç†è§£æ•°æ®é•¿ä»€ä¹ˆæ ·å­ï¼Œä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºã€‚</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel, <span class="hljs-type">Sequence</span>
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> display, HTML

<span class="hljs-keyword">def</span> <span class="hljs-title function_">show_random_elements</span>(<span class="hljs-params">dataset, num_examples=<span class="hljs-number">10</span></span>):
    <span class="hljs-keyword">assert</span> num_examples &lt;= <span class="hljs-built_in">len</span>(dataset), <span class="hljs-string">"Can't pick more elements than there are in the dataset."</span>
    picks = []
    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_examples):
        pick = random.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset)-<span class="hljs-number">1</span>)
        <span class="hljs-keyword">while</span> pick <span class="hljs-keyword">in</span> picks:
            pick = random.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset)-<span class="hljs-number">1</span>)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    <span class="hljs-keyword">for</span> column, typ <span class="hljs-keyword">in</span> dataset.features.items():
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(typ, ClassLabel):
            df[column] = df[column].transform(<span class="hljs-keyword">lambda</span> i: typ.names[i])
        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(typ, <span class="hljs-type">Sequence</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">isinstance</span>(typ.feature, ClassLabel):
            df[column] = df[column].transform(<span class="hljs-keyword">lambda</span> x: [typ.feature.names[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x])
    display(HTML(df.to_html()))
</code></pre>
<pre><code class="lang-python">show_random_elements(datasets[<span class="hljs-string">"train"</span>])
</code></pre>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>tokens</th>
      <th>pos_tags</th>
      <th>chunk_tags</th>
      <th>ner_tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2227</td>
      <td>[Result, of, a, French, first, division, match, on, Friday, .]</td>
      <td>[NN, IN, DT, JJ, JJ, NN, NN, IN, NNP, .]</td>
      <td>[B-NP, B-PP, B-NP, I-NP, I-NP, I-NP, I-NP, B-PP, B-NP, O]</td>
      <td>[O, O, O, B-MISC, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2615</td>
      <td>[Mid-tier, golds, up, in, heavy, trading, .]</td>
      <td>[NN, NNS, IN, IN, JJ, NN, .]</td>
      <td>[B-NP, I-NP, B-PP, B-PP, B-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10256</td>
      <td>[Neagle, (, 14-6, ), beat, the, Braves, for, the, third, time, this, season, ,, allowing, two, runs, and, six, hits, in, eight, innings, .]</td>
      <td>[NNP, (, CD, ), VB, DT, NNPS, IN, DT, JJ, NN, DT, NN, ,, VBG, CD, NNS, CC, CD, NNS, IN, CD, NN, .]</td>
      <td>[B-NP, O, B-NP, O, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, I-NP, B-NP, I-NP, O, B-VP, B-NP, I-NP, O, B-NP, I-NP, B-PP, B-NP, I-NP, O]</td>
      <td>[B-PER, O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10720</td>
      <td>[Hansa, Rostock, 4, 1, 2, 1, 5, 4, 5]</td>
      <td>[NNP, NNP, CD, CD, CD, CD, CD, CD, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7125</td>
      <td>[MONTREAL, 70, 59, .543, 11]</td>
      <td>[NNP, CD, CD, CD, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, O, O, O, O]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3316</td>
      <td>[Softbank, Corp, said, on, Friday, that, it, would, procure, $, 900, million, through, the, foreign, exchange, market, by, September, 5, as, part, of, its, acquisition, of, U.S., firm, ,, Kingston, Technology, Co, .]</td>
      <td>[NNP, NNP, VBD, IN, NNP, IN, PRP, MD, NN, $, CD, CD, IN, DT, JJ, NN, NN, IN, NNP, CD, IN, NN, IN, PRP$, NN, IN, NNP, NN, ,, NNP, NNP, NNP, .]</td>
      <td>[B-NP, I-NP, B-VP, B-PP, B-NP, B-SBAR, B-NP, B-VP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, O, B-NP, I-NP, I-NP, O]</td>
      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, B-ORG, I-ORG, I-ORG, O]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3923</td>
      <td>[Ghent, 3, Aalst, 2]</td>
      <td>[NN, CD, NNP, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, O, B-ORG, O]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2776</td>
      <td>[The, separatists, ,, who, swept, into, Grozny, on, August, 6, ,, still, control, large, areas, of, the, centre, of, town, ,, and, Russian, soldiers, are, based, at, checkpoints, on, the, approach, roads, .]</td>
      <td>[DT, NNS, ,, WP, VBD, IN, NNP, IN, NNP, CD, ,, RB, VBP, JJ, NNS, IN, DT, NN, IN, NN, ,, CC, JJ, NNS, VBP, VBN, IN, NNS, IN, DT, NN, NNS, .]</td>
      <td>[B-NP, I-NP, O, B-NP, B-VP, B-PP, B-NP, B-PP, B-NP, I-NP, O, B-ADVP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, O, O, B-NP, I-NP, B-VP, I-VP, B-PP, B-NP, B-PP, B-NP, I-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1178</td>
      <td>[Doctor, Masserigne, Ndiaye, said, medical, staff, were, overwhelmed, with, work, ., "]</td>
      <td>[NNP, NNP, NNP, VBD, JJ, NN, VBD, VBN, IN, NN, ., "]</td>
      <td>[B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-VP, I-VP, B-PP, B-NP, O, O]</td>
      <td>[O, B-PER, I-PER, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10988</td>
      <td>[Reuters, historical, calendar, -, September, 4, .]</td>
      <td>[NNP, JJ, NN, :, NNP, CD, .]</td>
      <td>[B-NP, I-NP, I-NP, O, B-NP, I-NP, O]</td>
      <td>[B-ORG, O, O, O, O, O, O]</td>
    </tr>
  </tbody>
</table>


<h2 id="é¢„å¤„ç†æ•°æ®">é¢„å¤„ç†æ•°æ®</h2>
<p>åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚é¢„å¤„ç†çš„å·¥å…·å«<code>Tokenizer</code>ã€‚<code>Tokenizer</code>é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚</p>
<p>ä¸ºäº†è¾¾åˆ°æ•°æ®é¢„å¤„ç†çš„ç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨<code>AutoTokenizer.from_pretrained</code>æ–¹æ³•å®ä¾‹åŒ–æˆ‘ä»¬çš„tokenizerï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š</p>
<ul>
<li>æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€ä¸€å¯¹åº”çš„tokenizerã€‚</li>
<li>ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹checkpointå¯¹åº”çš„tokenizerçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¹Ÿä¸‹è½½äº†æ¨¡å‹éœ€è¦çš„è¯è¡¨åº“vocabularyï¼Œå‡†ç¡®æ¥è¯´æ˜¯tokens vocabularyã€‚</li>
</ul>
<p>è¿™ä¸ªè¢«ä¸‹è½½çš„tokens vocabularyä¼šè¢«ç¼“å­˜èµ·æ¥ï¼Œä»è€Œå†æ¬¡ä½¿ç”¨çš„æ—¶å€™ä¸ä¼šé‡æ–°ä¸‹è½½ã€‚</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
</code></pre>
<p>æ³¨æ„ï¼šä»¥ä¸‹ä»£ç è¦æ±‚tokenizerå¿…é¡»æ˜¯transformers.PreTrainedTokenizerFastç±»å‹ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨é¢„å¤„ç†çš„æ—¶å€™éœ€è¦ç”¨åˆ°fast tokenizerçš„ä¸€äº›ç‰¹æ®Šç‰¹æ€§ï¼ˆæ¯”å¦‚å¤šçº¿ç¨‹å¿«é€Ÿtokenizerï¼‰ã€‚</p>
<p>å‡ ä¹æ‰€æœ‰æ¨¡å‹å¯¹åº”çš„tokenizeréƒ½æœ‰å¯¹åº”çš„fast tokenizerã€‚æˆ‘ä»¬å¯ä»¥åœ¨<a href="https://huggingface.co/transformers/index.html#bigtable" target="_blank">æ¨¡å‹tokenizerå¯¹åº”è¡¨</a>é‡ŒæŸ¥çœ‹æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„tokenizeræ‰€æ‹¥æœ‰çš„ç‰¹ç‚¹ã€‚</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> transformers
<span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(tokenizer, transformers.PreTrainedTokenizerFast)
</code></pre>
<p>åœ¨<a href="https://huggingface.co/transformers/index.html#bigtable" target="_blank">è¿™é‡Œbig table of models</a>æŸ¥çœ‹æ¨¡å‹æ˜¯å¦æœ‰fast tokenizerã€‚</p>
<p>tokenizeræ—¢å¯ä»¥å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œä¹Ÿå¯ä»¥å¯¹ä¸€å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œtokenizeré¢„å¤„ç†åå¾—åˆ°çš„æ•°æ®æ»¡è¶³é¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼</p>
<pre><code class="lang-python">tokenizer(<span class="hljs-string">"Hello, this is one sentence!"</span>)
</code></pre>
<pre><code>{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre><pre><code class="lang-python">tokenizer([<span class="hljs-string">"Hello"</span>, <span class="hljs-string">","</span>, <span class="hljs-string">"this"</span>, <span class="hljs-string">"is"</span>, <span class="hljs-string">"one"</span>, <span class="hljs-string">"sentence"</span>, <span class="hljs-string">"split"</span>, <span class="hljs-string">"into"</span>, <span class="hljs-string">"words"</span>, <span class="hljs-string">"."</span>], is_split_into_words=<span class="hljs-literal">True</span>)
</code></pre>
<pre><code>{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 3975, 2046, 2616, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre><p>æ³¨æ„transformeré¢„è®­ç»ƒæ¨¡å‹åœ¨é¢„è®­ç»ƒçš„æ—¶å€™é€šå¸¸ä½¿ç”¨çš„æ˜¯subwordï¼Œå¦‚æœæˆ‘ä»¬çš„æ–‡æœ¬è¾“å…¥å·²ç»è¢«åˆ‡åˆ†æˆäº†wordï¼Œé‚£ä¹ˆè¿™äº›wordè¿˜ä¼šè¢«æˆ‘ä»¬çš„tokenizerç»§ç»­åˆ‡åˆ†ã€‚ä¸¾ä¸ªä¾‹å­ï¼š</p>
<pre><code class="lang-python">example = datasets[<span class="hljs-string">"train"</span>][<span class="hljs-number">4</span>]
<span class="hljs-built_in">print</span>(example[<span class="hljs-string">"tokens"</span>])
</code></pre>
<pre><code>['Germany', "'s", 'representative', 'to', 'the', 'European', 'Union', "'s", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']
</code></pre><pre><code class="lang-python">tokenized_input = tokenizer(example[<span class="hljs-string">"tokens"</span>], is_split_into_words=<span class="hljs-literal">True</span>)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input[<span class="hljs-string">"input_ids"</span>])
<span class="hljs-built_in">print</span>(tokens)
</code></pre>
<pre><code>['[CLS]', 'germany', "'", 's', 'representative', 'to', 'the', 'european', 'union', "'", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']
</code></pre><p>å•è¯"Zwingmann" å’Œ "sheepmeat"ç»§ç»­è¢«åˆ‡åˆ†æˆäº†3ä¸ªsubtokensã€‚</p>
<p>ç”±äºæ ‡æ³¨æ•°æ®é€šå¸¸æ˜¯åœ¨wordçº§åˆ«è¿›è¡Œæ ‡æ³¨çš„ï¼Œæ—¢ç„¶wordè¿˜ä¼šè¢«åˆ‡åˆ†æˆsubtokensï¼Œé‚£ä¹ˆæ„å‘³ç€æˆ‘ä»¬è¿˜éœ€è¦å¯¹æ ‡æ³¨æ•°æ®è¿›è¡Œsubtokensçš„å¯¹é½ã€‚åŒæ—¶ï¼Œç”±äºé¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼çš„è¦æ±‚ï¼Œå¾€å¾€è¿˜éœ€è¦åŠ ä¸Šä¸€äº›ç‰¹æ®Šç¬¦å·æ¯”å¦‚ï¼š <code>[CLS]</code> å’Œ  <code>[SEP]</code>ã€‚</p>
<pre><code class="lang-python"><span class="hljs-built_in">len</span>(example[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>]), <span class="hljs-built_in">len</span>(tokenized_input[<span class="hljs-string">"input_ids"</span>])
</code></pre>
<pre><code>(31, 39)
</code></pre><p>tokenizeræœ‰ä¸€ä¸ª<code>`word_ids</code>æ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>
<pre><code class="lang-python"><span class="hljs-built_in">print</span>(tokenized_input.word_ids())
</code></pre>
<pre><code>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]
</code></pre><p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œword_idså°†æ¯ä¸€ä¸ªsubtokensä½ç½®éƒ½å¯¹åº”äº†ä¸€ä¸ªwordçš„ä¸‹æ ‡ã€‚æ¯”å¦‚ç¬¬1ä¸ªä½ç½®å¯¹åº”ç¬¬0ä¸ªwordï¼Œç„¶åç¬¬2ã€3ä¸ªä½ç½®å¯¹åº”ç¬¬1ä¸ªwordã€‚ç‰¹æ®Šå­—ç¬¦å¯¹åº”äº†Noneã€‚æœ‰äº†è¿™ä¸ªlistï¼Œæˆ‘ä»¬å°±èƒ½å°†subtokenså’Œwordsè¿˜æœ‰æ ‡æ³¨çš„labelså¯¹é½å•¦ã€‚</p>
<pre><code class="lang-python">word_ids = tokenized_input.word_ids()
aligned_labels = [-<span class="hljs-number">100</span> <span class="hljs-keyword">if</span> i <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> example[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> word_ids]
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(aligned_labels), <span class="hljs-built_in">len</span>(tokenized_input[<span class="hljs-string">"input_ids"</span>]))
</code></pre>
<pre><code>39 39
</code></pre><p>æˆ‘ä»¬é€šå¸¸å°†ç‰¹æ®Šå­—ç¬¦çš„labelè®¾ç½®ä¸º-100ï¼Œåœ¨æ¨¡å‹ä¸­-100é€šå¸¸ä¼šè¢«å¿½ç•¥æ‰ä¸è®¡ç®—lossã€‚</p>
<p>æˆ‘ä»¬æœ‰ä¸¤ç§å¯¹é½labelçš„æ–¹å¼ï¼š</p>
<ul>
<li>å¤šä¸ªsubtokenså¯¹é½ä¸€ä¸ªwordï¼Œå¯¹é½ä¸€ä¸ªlabel</li>
<li>å¤šä¸ªsubtokensçš„ç¬¬ä¸€ä¸ªsubtokenå¯¹é½wordï¼Œå¯¹é½ä¸€ä¸ªlabelï¼Œå…¶ä»–subtokensç›´æ¥èµ‹äºˆ-100.</li>
</ul>
<p>æˆ‘ä»¬æä¾›è¿™ä¸¤ç§æ–¹å¼ï¼Œé€šè¿‡<code>label_all_tokens = True</code>åˆ‡æ¢ã€‚</p>
<pre><code class="lang-python">label_all_tokens = <span class="hljs-literal">True</span>
</code></pre>
<p>æœ€åæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹åˆèµ·æ¥å˜æˆæˆ‘ä»¬çš„é¢„å¤„ç†å‡½æ•°ã€‚<code>is_split_into_words=True</code>åœ¨ä¸Šé¢å·²ç»ç»“æŸå•¦ã€‚</p>
<pre><code class="lang-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_and_align_labels</span>(<span class="hljs-params">examples</span>):
    tokenized_inputs = tokenizer(examples[<span class="hljs-string">"tokens"</span>], truncation=<span class="hljs-literal">True</span>, is_split_into_words=<span class="hljs-literal">True</span>)

    labels = []
    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(examples[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = <span class="hljs-literal">None</span>
        label_ids = []
        <span class="hljs-keyword">for</span> word_idx <span class="hljs-keyword">in</span> word_ids:
            <span class="hljs-comment"># Special tokens have a word id that is None. We set the label to -100 so they are automatically</span>
            <span class="hljs-comment"># ignored in the loss function.</span>
            <span class="hljs-keyword">if</span> word_idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
                label_ids.append(-<span class="hljs-number">100</span>)
            <span class="hljs-comment"># We set the label for the first token of each word.</span>
            <span class="hljs-keyword">elif</span> word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            <span class="hljs-comment"># For the other tokens in a word, we set the label to either the current label or -100, depending on</span>
            <span class="hljs-comment"># the label_all_tokens flag.</span>
            <span class="hljs-keyword">else</span>:
                label_ids.append(label[word_idx] <span class="hljs-keyword">if</span> label_all_tokens <span class="hljs-keyword">else</span> -<span class="hljs-number">100</span>)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs[<span class="hljs-string">"labels"</span>] = labels
    <span class="hljs-keyword">return</span> tokenized_inputs
</code></pre>
<p>ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚</p>
<pre><code class="lang-python">tokenize_and_align_labels(datasets[<span class="hljs-string">'train'</span>][:<span class="hljs-number">5</span>])
</code></pre>
<pre><code>{'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}
</code></pre><p>æ¥ä¸‹æ¥å¯¹æ•°æ®é›†datasetsé‡Œé¢çš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå¤„ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨mapå‡½æ•°ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚</p>
<pre><code class="lang-python">tokenized_datasets = datasets.<span class="hljs-built_in">map</span>(tokenize_and_align_labels, batched=<span class="hljs-literal">True</span>)
</code></pre>
<p>æ›´å¥½çš„æ˜¯ï¼Œè¿”å›çš„ç»“æœä¼šè‡ªåŠ¨è¢«ç¼“å­˜ï¼Œé¿å…ä¸‹æ¬¡å¤„ç†çš„æ—¶å€™é‡æ–°è®¡ç®—ï¼ˆä½†æ˜¯ä¹Ÿè¦æ³¨æ„ï¼Œå¦‚æœè¾“å…¥æœ‰æ”¹åŠ¨ï¼Œå¯èƒ½ä¼šè¢«ç¼“å­˜å½±å“ï¼ï¼‰ã€‚datasetsåº“å‡½æ•°ä¼šå¯¹è¾“å…¥çš„å‚æ•°è¿›è¡Œæ£€æµ‹ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–ï¼Œå¦‚æœæ²¡æœ‰å˜åŒ–å°±ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå¦‚æœæœ‰å˜åŒ–å°±é‡æ–°å¤„ç†ã€‚ä½†å¦‚æœè¾“å…¥å‚æ•°ä¸å˜ï¼Œæƒ³æ”¹å˜è¾“å…¥çš„æ—¶å€™ï¼Œæœ€å¥½æ¸…ç†è°ƒè¿™ä¸ªç¼“å­˜ã€‚æ¸…ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨<code>load_from_cache_file=False</code>å‚æ•°ã€‚å¦å¤–ï¼Œä¸Šé¢ä½¿ç”¨åˆ°çš„<code>batched=True</code>è¿™ä¸ªå‚æ•°æ˜¯tokenizerçš„ç‰¹ç‚¹ï¼Œå› ä¸ºè¿™ä¼šä½¿ç”¨å¤šçº¿ç¨‹åŒæ—¶å¹¶è¡Œå¯¹è¾“å…¥è¿›è¡Œå¤„ç†ã€‚</p>
<h2 id="å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹">å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹</h2>
<p>æ—¢ç„¶æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚æ—¢ç„¶æˆ‘ä»¬æ˜¯åšseq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨<code>AutoModelForTokenClassification</code> è¿™ä¸ªç±»ã€‚å’Œtokenizerç›¸ä¼¼ï¼Œ<code>from_pretrained</code>æ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œå°±ä¸ä¼šé‡å¤ä¸‹è½½æ¨¡å‹å•¦ã€‚</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=<span class="hljs-built_in">len</span>(label_list))
</code></pre>
<pre><code>Downloading:   0%|          | 0.00/268M [00:00&lt;?, ?B/s]


Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre><p>ç”±äºæˆ‘ä»¬å¾®è°ƒçš„ä»»åŠ¡æ˜¯tokenåˆ†ç±»ä»»åŠ¡ï¼Œè€Œæˆ‘ä»¬åŠ è½½çš„æ˜¯é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ‰€ä»¥ä¼šæç¤ºæˆ‘ä»¬åŠ è½½æ¨¡å‹çš„æ—¶å€™æ‰”æ‰äº†ä¸€äº›ä¸åŒ¹é…çš„ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆæ¯”å¦‚ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œheadè¢«æ‰”æ‰äº†ï¼ŒåŒæ—¶éšæœºåˆå§‹åŒ–äº†tokenåˆ†ç±»çš„ç¥ç»ç½‘ç»œheadï¼‰ã€‚</p>
<p>ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª<code>Trainer</code>è®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦3ä¸ªè¦ç´ ï¼Œå…¶ä¸­æœ€é‡è¦çš„æ˜¯è®­ç»ƒçš„è®¾å®š/å‚æ•° <a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments" target="_blank"><code>TrainingArguments</code></a>ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§ã€‚</p>
<pre><code class="lang-python">args = TrainingArguments(
    <span class="hljs-string">f"test-<span class="hljs-subst">{task}</span>"</span>,
    evaluation_strategy = <span class="hljs-string">"epoch"</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)
</code></pre>
<p>ä¸Šé¢evaluation_strategy = "epoch"å‚æ•°å‘Šè¯‰è®­ç»ƒä»£ç ï¼šæˆ‘ä»¬æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°ã€‚</p>
<p>ä¸Šé¢batch_sizeåœ¨è¿™ä¸ªnotebookä¹‹å‰å®šä¹‰å¥½äº†ã€‚</p>
<p>æœ€åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†æˆ‘ä»¬å¤„ç†å¥½çš„è¾“å…¥å–‚ç»™æ¨¡å‹ã€‚</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer)
</code></pre>
<p>è®¾ç½®å¥½<code>Trainer</code>è¿˜å‰©æœ€åä¸€ä»¶äº‹æƒ…ï¼Œé‚£å°±æ˜¯æˆ‘ä»¬éœ€è¦å®šä¹‰å¥½è¯„ä¼°æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨<a href="https://github.com/chakki-works/seqeval" target="_blank"><code>seqeval</code></a> metricæ¥å®Œæˆè¯„ä¼°ã€‚å°†æ¨¡å‹é¢„æµ‹é€å…¥è¯„ä¼°ä¹‹å‰ï¼Œæˆ‘ä»¬ä¹Ÿä¼šåšä¸€äº›æ•°æ®åå¤„ç†ï¼š</p>
<pre><code class="lang-python">metric = load_metric(<span class="hljs-string">"seqeval"</span>)
</code></pre>
<p>è¯„ä¼°çš„è¾“å…¥æ˜¯é¢„æµ‹å’Œlabelçš„list</p>
<pre><code class="lang-python">labels = [label_list[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> example[<span class="hljs-string">f"<span class="hljs-subst">{task}</span>_tags"</span>]]
metric.compute(predictions=[labels], references=[labels])
</code></pre>
<pre><code>{'LOC': {'f1': 1.0, 'number': 2, 'precision': 1.0, 'recall': 1.0},
 'ORG': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},
 'PER': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},
 'overall_accuracy': 1.0,
 'overall_f1': 1.0,
 'overall_precision': 1.0,
 'overall_recall': 1.0}
</code></pre><p>å¯¹æ¨¡å‹é¢„æµ‹ç»“æœåšä¸€äº›åå¤„ç†ï¼š</p>
<ul>
<li>é€‰æ‹©é¢„æµ‹åˆ†ç±»æœ€å¤§æ¦‚ç‡çš„ä¸‹æ ‡</li>
<li>å°†ä¸‹æ ‡è½¬åŒ–ä¸ºlabel</li>
<li>å¿½ç•¥-100æ‰€åœ¨åœ°æ–¹</li>
</ul>
<p>ä¸‹é¢çš„å‡½æ•°å°†ä¸Šé¢çš„æ­¥éª¤åˆå¹¶äº†èµ·æ¥ã€‚</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">p</span>):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=<span class="hljs-number">2</span>)

    <span class="hljs-comment"># Remove ignored index (special tokens)</span>
    true_predictions = [
        [label_list[p] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
        <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
    ]
    true_labels = [
        [label_list[l] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
        <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">"precision"</span>: results[<span class="hljs-string">"overall_precision"</span>],
        <span class="hljs-string">"recall"</span>: results[<span class="hljs-string">"overall_recall"</span>],
        <span class="hljs-string">"f1"</span>: results[<span class="hljs-string">"overall_f1"</span>],
        <span class="hljs-string">"accuracy"</span>: results[<span class="hljs-string">"overall_accuracy"</span>],
    }
</code></pre>
<p>æˆ‘ä»¬è®¡ç®—æ‰€æœ‰ç±»åˆ«æ€»çš„precision/recall/f1ï¼Œæ‰€ä»¥ä¼šæ‰”æ‰å•ä¸ªç±»åˆ«çš„precision/recall/f1 </p>
<p>å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥<code>Trainer</code>å³å¯</p>
<pre><code class="lang-python">trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">"train"</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">"validation"</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
</code></pre>
<p>è°ƒç”¨<code>train</code>æ–¹æ³•å¼€å§‹è®­ç»ƒ</p>
<pre><code class="lang-python">trainer.train()
</code></pre>
<pre><code>&lt;div&gt;
    &lt;style&gt;
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
    &lt;/style&gt;

  &lt;progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'&gt;&lt;/progress&gt;
  [2634/2634 01:45, Epoch 3/3]
&lt;/div&gt;
&lt;table border="1" class="dataframe"&gt;
</code></pre><p>  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.237721</td>
      <td>0.068198</td>
      <td>0.903148</td>
      <td>0.921132</td>
      <td>0.912051</td>
      <td>0.979713</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.053160</td>
      <td>0.059337</td>
      <td>0.927697</td>
      <td>0.932990</td>
      <td>0.930336</td>
      <td>0.983113</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.029850</td>
      <td>0.059346</td>
      <td>0.929267</td>
      <td>0.939143</td>
      <td>0.934179</td>
      <td>0.984257</td>
    </tr>
  </tbody>
&lt;/table&gt;</p><p></p>
<pre><code>TrainOutput(global_step=2634, training_loss=0.08569671253227518)
</code></pre><p>æˆ‘ä»¬å¯ä»¥å†æ¬¡ä½¿ç”¨<code>evaluate</code>æ–¹æ³•è¯„ä¼°ï¼Œå¯ä»¥è¯„ä¼°å…¶ä»–æ•°æ®é›†ã€‚</p>
<pre><code class="lang-python">trainer.evaluate()
</code></pre>
<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
    </style>

  <progress value="408" max="204" style="width:300px; height:20px; vertical-align: middle;"></progress>
  [204/204 00:05]
</div>






<pre><code>{'eval_loss': 0.05934586375951767,
 'eval_precision': 0.9292672127518264,
 'eval_recall': 0.9391430808815304,
 'eval_f1': 0.9341790463472988,
 'eval_accuracy': 0.9842565968195466,
 'epoch': 3.0}
</code></pre><p>å¦‚æœæƒ³è¦å¾—åˆ°å•ä¸ªç±»åˆ«çš„precision/recall/f1ï¼Œæˆ‘ä»¬ç›´æ¥å°†ç»“æœè¾“å…¥ç›¸åŒçš„è¯„ä¼°å‡½æ•°å³å¯ï¼š</p>
<pre><code class="lang-python">predictions, labels, _ = trainer.predict(tokenized_datasets[<span class="hljs-string">"validation"</span>])
predictions = np.argmax(predictions, axis=<span class="hljs-number">2</span>)

<span class="hljs-comment"># Remove ignored index (special tokens)</span>
true_predictions = [
    [label_list[p] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
    <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
]
true_labels = [
    [label_list[l] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
    <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
]

results = metric.compute(predictions=true_predictions, references=true_labels)
results
</code></pre>
<pre><code>{'LOC': {'precision': 0.949718574108818,
  'recall': 0.966768525592055,
  'f1': 0.9581677077418134,
  'number': 2618},
 'MISC': {'precision': 0.8132387706855791,
  'recall': 0.8383428107229894,
  'f1': 0.8255999999999999,
  'number': 1231},
 'ORG': {'precision': 0.9055232558139535,
  'recall': 0.9090466926070039,
  'f1': 0.9072815533980583,
  'number': 2056},
 'PER': {'precision': 0.9759552042160737,
  'recall': 0.9765985497692815,
  'f1': 0.9762767710049424,
  'number': 3034},
 'overall_precision': 0.9292672127518264,
 'overall_recall': 0.9391430808815304,
 'overall_f1': 0.9341790463472988,
 'overall_accuracy': 0.9842565968195466}
</code></pre><p>æœ€ååˆ«å¿˜äº†ï¼Œä¸Šä¼ æ¨¡å‹åˆ°<a href="https://huggingface.co/models" target="_blank">ğŸ¤— Model Hub</a>ï¼ˆç‚¹å‡»<a href="https://huggingface.co/transformers/model_sharing.html" target="_blank">è¿™é‡Œ</a>æ¥æŸ¥çœ‹å¦‚ä½•ä¸Šä¼ ï¼‰ã€‚éšåæ‚¨å°±å¯ä»¥åƒè¿™ä¸ªnotebookä¸€å¼€å§‹ä¸€æ ·ï¼Œç›´æ¥ç”¨æ¨¡å‹åå­—å°±èƒ½ä½¿ç”¨æ‚¨è‡ªå·±ä¸Šä¼ çš„æ¨¡å‹å•¦ã€‚</p>
<pre><code class="lang-python">

</code></pre>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="4.1-æ–‡æœ¬åˆ†ç±».html" class="navigation navigation-prev " aria-label="Previous page: 4.1-æ–‡æœ¬åˆ†ç±»">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="4.3-é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”.html" class="navigation navigation-next " aria-label="Next page: 4.3-é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"4.2-åºåˆ—æ ‡æ³¨","level":"4.4","depth":1,"next":{"title":"4.3-é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”","level":"4.5","depth":1,"path":"ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.3-é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”.md","ref":"./ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.3-é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”.md","articles":[]},"previous":{"title":"4.1-æ–‡æœ¬åˆ†ç±»","level":"4.3","depth":1,"path":"ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.1-æ–‡æœ¬åˆ†ç±».md","ref":"./ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.1-æ–‡æœ¬åˆ†ç±».md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.2-åºåˆ—æ ‡æ³¨.md","mtime":"2024-08-23T15:34:37.387Z","type":"markdown"},"gitbook":{"version":"5.1.4","time":"2024-08-23T15:50:04.957Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

