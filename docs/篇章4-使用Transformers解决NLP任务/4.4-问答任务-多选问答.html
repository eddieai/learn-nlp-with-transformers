
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <title>4.4-问答任务-多选问答 · HonKit</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="HonKit 5.1.4">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/@honkit/honkit-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, user-scalable=yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="4.5-生成任务-语言模型.html" />
    
    
    <link rel="prev" href="4.3-问答任务-抽取式问答.html" />
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">篇章1-前言</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../篇章1-前言/1.0-本地阅读和代码运行环境配置.html">
            
                <a href="../篇章1-前言/1.0-本地阅读和代码运行环境配置.html">
            
                    
                    1.0-本地阅读和代码运行环境配置
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../篇章1-前言/1.1-Transformers在NLP中的兴起.html">
            
                <a href="../篇章1-前言/1.1-Transformers在NLP中的兴起.html">
            
                    
                    1.1-Transformers在NLP中的兴起
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">篇章2-Transformer相关原理</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../篇章2-Transformer相关原理/2.0-前言.html">
            
                <a href="../篇章2-Transformer相关原理/2.0-前言.html">
            
                    
                    2.0-前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../篇章2-Transformer相关原理/2.1-图解attention.html">
            
                <a href="../篇章2-Transformer相关原理/2.1-图解attention.html">
            
                    
                    2.1-图解attention
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../篇章2-Transformer相关原理/2.2-图解transformer.html">
            
                <a href="../篇章2-Transformer相关原理/2.2-图解transformer.html">
            
                    
                    2.2-图解transformer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="../篇章2-Transformer相关原理/2.2.1-Pytorch编写Transformer.html">
            
                <a href="../篇章2-Transformer相关原理/2.2.1-Pytorch编写Transformer.html">
            
                    
                    2.2.1-Pytorch编写Transformer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.5" data-path="../篇章2-Transformer相关原理/2.2.1-Pytorch编写Transformer-选读.md">
            
                <span>
            
                    
                    2.2.2-Pytorch编写Transformer-选读
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.6" data-path="../篇章2-Transformer相关原理/2.3-图解BERT.html">
            
                <a href="../篇章2-Transformer相关原理/2.3-图解BERT.html">
            
                    
                    2.3-图解BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.7" data-path="../篇章2-Transformer相关原理/2.4-图解GPT.html">
            
                <a href="../篇章2-Transformer相关原理/2.4-图解GPT.html">
            
                    
                    2.4-图解GPT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.8" data-path="../篇章2-Transformer相关原理/2.5-篇章小测.html">
            
                <a href="../篇章2-Transformer相关原理/2.5-篇章小测.html">
            
                    
                    2.5-篇章小测
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">篇章3-编写一个Transformer模型：BERT</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../篇章3-编写一个Transformer模型：BERT/3.1-如何实现一个BERT.html">
            
                <a href="../篇章3-编写一个Transformer模型：BERT/3.1-如何实现一个BERT.html">
            
                    
                    3.1-如何实现一个BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="../篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT.html">
            
                <a href="../篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT.html">
            
                    
                    3.2-如何应用一个BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="../篇章3-编写一个Transformer模型：BERT/3.3-篇章小测.html">
            
                <a href="../篇章3-编写一个Transformer模型：BERT/3.3-篇章小测.html">
            
                    
                    3.3-篇章小测
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">篇章4-使用Transformers解决NLP任务</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="4.0-前言.html">
            
                <a href="4.0-前言.html">
            
                    
                    4.0-前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="4.0-基于HuggingFace-Transformers的预训练模型微调.html">
            
                <a href="4.0-基于HuggingFace-Transformers的预训练模型微调.html">
            
                    
                    4.0-基于HuggingFace-Transformers的预训练模型微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="4.1-文本分类.html">
            
                <a href="4.1-文本分类.html">
            
                    
                    4.1-文本分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="4.2-序列标注.html">
            
                <a href="4.2-序列标注.html">
            
                    
                    4.2-序列标注
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="4.3-问答任务-抽取式问答.html">
            
                <a href="4.3-问答任务-抽取式问答.html">
            
                    
                    4.3-问答任务-抽取式问答
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="4.6" data-path="4.4-问答任务-多选问答.html">
            
                <a href="4.4-问答任务-多选问答.html">
            
                    
                    4.4-问答任务-多选问答
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.7" data-path="4.5-生成任务-语言模型.html">
            
                <a href="4.5-生成任务-语言模型.html">
            
                    
                    4.5-生成任务-语言模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.8" data-path="4.6-生成任务-机器翻译.html">
            
                <a href="4.6-生成任务-机器翻译.html">
            
                    
                    4.6-生成任务-机器翻译
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.9" data-path="4.7-生成任务-摘要生成.html">
            
                <a href="4.7-生成任务-摘要生成.html">
            
                    
                    4.7-生成任务-摘要生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10" data-path="4.8-篇章小测.html">
            
                <a href="4.8-篇章小测.html">
            
                    
                    4.8-篇章小测
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://github.com/honkit/honkit" target="blank" class="gitbook-link">
            Published with HonKit
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >4.4-问答任务-多选问答</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <p>本文涉及的jupter notebook在<a href="https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1" target="_blank">篇章4代码库中</a>。</p>
<p>如果您在colab上打开这个jupyter笔记本，您需要安装🤗Trasnformers和🤗datasets。具体命令如下（取消注释并运行，如果速度慢请切换国内源，加上第二行的参数）。</p>
<p>在运行单元格之前，建议您按照本项目readme中提示，建立一个专门的python环境用于学习。</p>
<pre><code class="lang-python"><span class="hljs-comment">#! pip install datasets transformers </span>
<span class="hljs-comment"># -i https://pypi.tuna.tsinghua.edu.cn/simple</span>
</code></pre>
<p>如果您是在本地机器上打开这个jupyter笔记本，请确保您的环境安装了上述库的最新版本。</p>
<p>您可以在<a href="https://github.com/huggingface/transformers/blob/master/examples/pytorch/multiple-choice/" target="_blank">这里</a>找到这个jupyter笔记本的具体的python脚本文件，还可以通过分布式的方式使用多个gpu或tpu来微调您的模型。</p>
<h1 id="通过微调模型构建多选任务">通过微调模型构建多选任务</h1>
<p>在当前jupyter笔记本中，我们将说明如何通过微调任意<a href="https://github.com/huggingface/transformers" target="_blank">🤗Transformers</a> 模型来构建多选任务，该任务是在给定的多个答案中选择最合理的一个。我们使用的数据集是<a href="https://www.aclweb.org/anthology/D18-1009/" target="_blank">SWAG</a>，当然你也可以将预处理过程用于其他多选数据集或者你自己的数据。SWAG是一个关于常识推理的数据集，每个样本描述一种情况，然后给出四个可能的选项。</p>
<p>这个jupyter笔记本可以运行在<a href="https://huggingface.co/models" target="_blank">model Hub</a>中的任何模型上，只要该模型具有一个多选择头的版本。根据你的模型和你使用的GPU，你可能需要调整批大小，以避免显存不足的错误。设置好这两个参数之后，jupyter笔记本的其余部分就可以顺利运行了:</p>
<pre><code class="lang-python">model_checkpoint = <span class="hljs-string">"bert-base-uncased"</span>
batch_size = <span class="hljs-number">16</span>
</code></pre>
<h2 id="加载数据集">加载数据集</h2>
<p>我们将使用<a href="https://github.com/huggingface/datasets" target="_blank">🤗Datasets</a>库来下载数据。这一过程可以很容易地用函数<code>load_dataset</code>来完成。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
</code></pre>
<p><code>load_dataset</code> 将缓存数据集以避免下次运行时再次下载它。</p>
<pre><code class="lang-python">datasets = load_dataset(<span class="hljs-string">"swag"</span>, <span class="hljs-string">"regular"</span>)
</code></pre>
<pre><code>Reusing dataset swag (/home/sgugger/.cache/huggingface/datasets/swag/regular/0.0.0/f9784740e0964a3c799d68cec0d992cc267d3fe94f3e048175eca69d739b980d)
</code></pre><p>除此之外，你也可以从我们提供的<a href="https://gas.graviti.cn/dataset/datawhale/SWAG" target="_blank">链接</a>下载数据并解压，将解压后的3个csv文件复制到到<code>docs/篇章4-使用Transformers解决NLP任务/datasets/swag</code>目录下，然后用下面的代码进行加载。</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> os

data_path = <span class="hljs-string">'./datasets/swag/'</span>
cache_dir = os.path.join(data_path, <span class="hljs-string">'cache'</span>)
data_files = {<span class="hljs-string">'train'</span>: os.path.join(data_path, <span class="hljs-string">'train.csv'</span>), <span class="hljs-string">'val'</span>: os.path.join(data_path, <span class="hljs-string">'val.csv'</span>), <span class="hljs-string">'test'</span>: os.path.join(data_path, <span class="hljs-string">'test.csv'</span>)}
datasets = load_dataset(data_path, <span class="hljs-string">'regular'</span>, data_files=data_files, cache_dir=cache_dir)
</code></pre>
<pre><code>Using custom data configuration regular-2ab2d66f12115abf


Downloading and preparing dataset swag/regular (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to ./datasets/swag/cache/swag/regular-2ab2d66f12115abf/0.0.0/a16ae67faa24f4cdd6d1fc6bfc09bdb6dc15771716221ff8bacbc6cc75533614...




Dataset swag downloaded and prepared to ./datasets/swag/cache/swag/regular-2ab2d66f12115abf/0.0.0/a16ae67faa24f4cdd6d1fc6bfc09bdb6dc15771716221ff8bacbc6cc75533614. Subsequent calls will reuse this data.
</code></pre><p><code>dataset</code>对象本身是<a href="https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict" target="_blank"><code>DatasetDict</code></a>，它包含用于训练、验证和测试集的键值对(<code>mnli</code>是一个特殊的例子，其中包含用于不匹配的验证和测试集的键值对)。</p>
<pre><code class="lang-python">datasets
</code></pre>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],
        num_rows: 73546
    })
    validation: Dataset({
        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],
        num_rows: 20006
    })
    test: Dataset({
        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],
        num_rows: 20005
    })
})
</code></pre><p>To access an actual element, you need to select a split first, then give an index:</p>
<pre><code class="lang-python">datasets[<span class="hljs-string">"train"</span>][<span class="hljs-number">0</span>]
</code></pre>
<pre><code>{'ending0': 'passes by walking down the street playing their instruments.',
 'ending1': 'has heard approaching them.',
 'ending2': "arrives and they're outside dancing and asleep.",
 'ending3': 'turns the lead singer watches the performance.',
 'fold-ind': '3416',
 'gold-source': 'gold',
 'label': 0,
 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',
 'sent2': 'A drum line',
 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',
 'video-id': 'anetv_jkn6uvmqwh4'}
</code></pre><p>为了了解数据是什么样子的，下面的函数将显示数据集中随机选取的一些示例。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> display, HTML

<span class="hljs-keyword">def</span> <span class="hljs-title function_">show_random_elements</span>(<span class="hljs-params">dataset, num_examples=<span class="hljs-number">10</span></span>):
    <span class="hljs-keyword">assert</span> num_examples &lt;= <span class="hljs-built_in">len</span>(dataset), <span class="hljs-string">"Can't pick more elements than there are in the dataset."</span>
    picks = []
    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_examples):
        pick = random.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset)-<span class="hljs-number">1</span>)
        <span class="hljs-keyword">while</span> pick <span class="hljs-keyword">in</span> picks:
            pick = random.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset)-<span class="hljs-number">1</span>)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    <span class="hljs-keyword">for</span> column, typ <span class="hljs-keyword">in</span> dataset.features.items():
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(typ, ClassLabel):
            df[column] = df[column].transform(<span class="hljs-keyword">lambda</span> i: typ.names[i])
    display(HTML(df.to_html()))
</code></pre>
<pre><code class="lang-python">show_random_elements(datasets[<span class="hljs-string">"train"</span>])
</code></pre>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ending0</th>
      <th>ending1</th>
      <th>ending2</th>
      <th>ending3</th>
      <th>fold-ind</th>
      <th>gold-source</th>
      <th>label</th>
      <th>sent1</th>
      <th>sent2</th>
      <th>startphrase</th>
      <th>video-id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>are seated on a field.</td>
      <td>are skiing down the slope.</td>
      <td>are in a lift.</td>
      <td>are pouring out in a man.</td>
      <td>16668</td>
      <td>gold</td>
      <td>1</td>
      <td>A man is wiping the skiboard.</td>
      <td>Group of people</td>
      <td>A man is wiping the skiboard. Group of people</td>
      <td>anetv_JmL6BiuXr_g</td>
    </tr>
    <tr>
      <th>1</th>
      <td>performs stunts inside a gym.</td>
      <td>shows several shopping in the water.</td>
      <td>continues his skateboard while talking.</td>
      <td>is putting a black bike close.</td>
      <td>11424</td>
      <td>gold</td>
      <td>0</td>
      <td>The credits of the video are shown.</td>
      <td>A lady</td>
      <td>The credits of the video are shown. A lady</td>
      <td>anetv_dWyE0o2NetQ</td>
    </tr>
    <tr>
      <th>2</th>
      <td>is emerging into the hospital.</td>
      <td>are strewn under water at some wreckage.</td>
      <td>tosses the wand together and saunters into the marketplace.</td>
      <td>swats him upside down.</td>
      <td>15023</td>
      <td>gen</td>
      <td>1</td>
      <td>Through his binoculars, someone watches a handful of surfers being rolled up into the wave.</td>
      <td>Someone</td>
      <td>Through his binoculars, someone watches a handful of surfers being rolled up into the wave. Someone</td>
      <td>lsmdc3016_CHASING_MAVERICKS-6791</td>
    </tr>
    <tr>
      <th>3</th>
      <td>spies someone sitting below.</td>
      <td>opens the fridge and checks out the photo.</td>
      <td>puts a little sheepishly.</td>
      <td>staggers up to him.</td>
      <td>5475</td>
      <td>gold</td>
      <td>3</td>
      <td>He tips it upside down, and its little umbrella falls to the floor.</td>
      <td>Back inside, someone</td>
      <td>He tips it upside down, and its little umbrella falls to the floor. Back inside, someone</td>
      <td>lsmdc1008_Spider-Man2-75503</td>
    </tr>
    <tr>
      <th>4</th>
      <td>carries her to the grave.</td>
      <td>laughs as someone styles her hair.</td>
      <td>sets down his glass.</td>
      <td>stares after her then trudges back up into the street.</td>
      <td>6904</td>
      <td>gen</td>
      <td>1</td>
      <td>Someone kisses her smiling daughter on the cheek and beams back at the camera.</td>
      <td>Someone</td>
      <td>Someone kisses her smiling daughter on the cheek and beams back at the camera. Someone</td>
      <td>lsmdc1028_No_Reservations-83242</td>
    </tr>
    <tr>
      <th>5</th>
      <td>stops someone and sweeps all the way back from the lower deck to join them.</td>
      <td>is being dragged towards the monstrous animation.</td>
      <td>beats out many events at the touch of the sword, crawling it.</td>
      <td>reaches into a pocket and yanks open the door.</td>
      <td>14089</td>
      <td>gen</td>
      <td>1</td>
      <td>But before he can use his wand, he accidentally rams it up the troll's nostril.</td>
      <td>The angry troll</td>
      <td>But before he can use his wand, he accidentally rams it up the troll's nostril. The angry troll</td>
      <td>lsmdc1053_Harry_Potter_and_the_philosophers_stone-95867</td>
    </tr>
    <tr>
      <th>6</th>
      <td>sees someone's name in the photo.</td>
      <td>gives a surprised look.</td>
      <td>kneels down and touches his ripped specs.</td>
      <td>spies on someone's clock.</td>
      <td>8407</td>
      <td>gen</td>
      <td>1</td>
      <td>Someone keeps his tired eyes on the road.</td>
      <td>Glancing over, he</td>
      <td>Someone keeps his tired eyes on the road. Glancing over, he</td>
      <td>lsmdc1024_Identity_Thief-82693</td>
    </tr>
    <tr>
      <th>7</th>
      <td>stops as someone speaks into the camera.</td>
      <td>notices how blue his eyes are.</td>
      <td>is flung out of the door and knocks the boy over.</td>
      <td>flies through the air, its a fireball.</td>
      <td>4523</td>
      <td>gold</td>
      <td>1</td>
      <td>Both people are knocked back a few steps from the force of the collision.</td>
      <td>She</td>
      <td>Both people are knocked back a few steps from the force of the collision. She</td>
      <td>lsmdc0043_Thelma_and_Luise-68271</td>
    </tr>
    <tr>
      <th>8</th>
      <td>sits close to the river.</td>
      <td>have pet's supplies and pets.</td>
      <td>pops parked outside the dirt facility, sending up a car highway to catch control.</td>
      <td>displays all kinds of power tools and website.</td>
      <td>8112</td>
      <td>gold</td>
      <td>1</td>
      <td>A guy waits in the waiting room with his pet.</td>
      <td>A pet store and its van</td>
      <td>A guy waits in the waiting room with his pet. A pet store and its van</td>
      <td>anetv_9VWoQpg9wqE</td>
    </tr>
    <tr>
      <th>9</th>
      <td>the slender someone, someone turns on the light.</td>
      <td>, someone gives them to her boss then dumps some alcohol into dough.</td>
      <td>liquids from a bowl, she slams them drunk.</td>
      <td>wags his tail as someone returns to the hotel room.</td>
      <td>10867</td>
      <td>gold</td>
      <td>3</td>
      <td>Inside a convenience store, she opens a freezer case.</td>
      <td>Dolce</td>
      <td>Inside a convenience store, she opens a freezer case. Dolce</td>
      <td>lsmdc3090_YOUNG_ADULT-43871</td>
    </tr>
  </tbody>
</table>


<p>数据集中的每个示例都有一个上下文，它是由第一个句子(字段<code>sent1</code>)和第二个句子的简介(字段<code>sent2</code>)组成。然后给出四种可能的结尾(字段<code>ending0</code>， <code>ending1</code>， <code>ending2</code>和<code>ending3</code>)，然后让模型从中选择正确的一个(由字段<code>label</code>表示)。下面的函数让我们更直观地看到一个示例:</p>
<pre><code class="lang-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_one</span>(<span class="hljs-params">example</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Context: <span class="hljs-subst">{example[<span class="hljs-string">'sent1'</span>]}</span>"</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"  A - <span class="hljs-subst">{example[<span class="hljs-string">'sent2'</span>]}</span> <span class="hljs-subst">{example[<span class="hljs-string">'ending0'</span>]}</span>"</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"  B - <span class="hljs-subst">{example[<span class="hljs-string">'sent2'</span>]}</span> <span class="hljs-subst">{example[<span class="hljs-string">'ending1'</span>]}</span>"</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"  C - <span class="hljs-subst">{example[<span class="hljs-string">'sent2'</span>]}</span> <span class="hljs-subst">{example[<span class="hljs-string">'ending2'</span>]}</span>"</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"  D - <span class="hljs-subst">{example[<span class="hljs-string">'sent2'</span>]}</span> <span class="hljs-subst">{example[<span class="hljs-string">'ending3'</span>]}</span>"</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"\nGround truth: option <span class="hljs-subst">{[<span class="hljs-string">'A'</span>, <span class="hljs-string">'B'</span>, <span class="hljs-string">'C'</span>, <span class="hljs-string">'D'</span>][example[<span class="hljs-string">'label'</span>]]}</span>"</span>)
</code></pre>
<pre><code class="lang-python">show_one(datasets[<span class="hljs-string">"train"</span>][<span class="hljs-number">0</span>])
</code></pre>
<pre><code>Context: Members of the procession walk down the street holding small horn brass instruments.
  A - A drum line passes by walking down the street playing their instruments.
  B - A drum line has heard approaching them.
  C - A drum line arrives and they're outside dancing and asleep.
  D - A drum line turns the lead singer watches the performance.

Ground truth: option A
</code></pre><pre><code class="lang-python">show_one(datasets[<span class="hljs-string">"train"</span>][<span class="hljs-number">15</span>])
</code></pre>
<pre><code>Context: Now it's someone's turn to rain blades on his opponent.
  A - Someone pats his shoulder and spins wildly.
  B - Someone lunges forward through the window.
  C - Someone falls to the ground.
  D - Someone rolls up his fast run from the water and tosses in the sky.

Ground truth: option C
</code></pre><h2 id="数据预处理">数据预处理</h2>
<p>在将这些文本输入到模型之前，我们需要对它们进行预处理。这是由🤗transformer的<code>Tokenizer</code>完成的，正如它的名字所暗示的那样，它将输入表示为一系列token，然后通过查找预训练好的词汇表，将它们转换为相应的id。最后转换成模型所期望的格式，同时生成模型所需的其他输入。</p>
<p>为了做到这一切，我们使用<code>AutoTokenizer</code>的<code>from_pretrained</code>方法实例化我们的tokenizer，它将确保:</p>
<p>-我们得到一个对应于我们想要使用的模型架构的tokenizer，
-我们下载好了预训练这个特定模型时使用的词表。</p>
<p>同时，该词表将被缓存，因此下次运行时不会再次下载它。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=<span class="hljs-literal">True</span>)
</code></pre>
<p>我们将<code>use_fast=True</code>作为参数入，以使用🤗tokenizers库中的一个快速tokenizer(它由Rust支持的)。这些快速tokenizer几乎适用于所有模型，但如果您在前面的调用中出现错误，请删除该参数。</p>
<p>你可以直接在一个句子或一个句子对上调用这个tokenizer:</p>
<pre><code class="lang-python">tokenizer(<span class="hljs-string">"Hello, this one sentence!"</span>, <span class="hljs-string">"And this sentence goes with it."</span>)
</code></pre>
<pre><code>{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre><p>根据您选择的模型，您将在上面单元格返回的字典中看到不同的键值对。它们对于我们在这里所做的并不重要，只需要知道它们是我们稍后实例化的模型所需要的。如果您对此感兴趣，可以在<a href="https://huggingface.co/transformers/preprocessing.html" target="_blank">本教程</a>中了解更多关于它们的信息。</p>
<p>如下面的字典所示，为了对数据集进行预处理，我们需要知道包含句子的列的名称:</p>
<p>我们可以写一个函数来预处理我们的样本。在调用tokenizer之前，最棘手的部分是将所有可能的句子对放在两个大列表中，然后将结果拉平，以便每个示例有四个输入id、注意力掩码等。</p>
<p>当调用<code>tokenizer</code>时，我们传入参数<code>truncation=True</code>。这将确保比所选模型所能处理的更长的输入将被截断为模型所能接受的最大长度。</p>
<pre><code class="lang-python">ending_names = [<span class="hljs-string">"ending0"</span>, <span class="hljs-string">"ending1"</span>, <span class="hljs-string">"ending2"</span>, <span class="hljs-string">"ending3"</span>]

<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-comment"># Repeat each first sentence four times to go with the four possibilities of second sentences.</span>
    first_sentences = [[context] * <span class="hljs-number">4</span> <span class="hljs-keyword">for</span> context <span class="hljs-keyword">in</span> examples[<span class="hljs-string">"sent1"</span>]]
    <span class="hljs-comment"># Grab all second sentences possible for each context.</span>
    question_headers = examples[<span class="hljs-string">"sent2"</span>]
    second_sentences = [[<span class="hljs-string">f"<span class="hljs-subst">{header}</span> <span class="hljs-subst">{examples[end][i]}</span>"</span> <span class="hljs-keyword">for</span> end <span class="hljs-keyword">in</span> ending_names] <span class="hljs-keyword">for</span> i, header <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(question_headers)]

    <span class="hljs-comment"># Flatten everything</span>
    first_sentences = <span class="hljs-built_in">sum</span>(first_sentences, [])
    second_sentences = <span class="hljs-built_in">sum</span>(second_sentences, [])

    <span class="hljs-comment"># Tokenize</span>
    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Un-flatten</span>
    <span class="hljs-keyword">return</span> {k: [v[i:i+<span class="hljs-number">4</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(v), <span class="hljs-number">4</span>)] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> tokenized_examples.items()}
</code></pre>
<p>This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists of lists for each key: a list of all examples (here 5), then a list of all choices (4) and a list of input IDs (length varying here since we did not apply any padding):</p>
<p>这个函数可以使用一个或多个示例。在传入多个示例时，tokenizer将为每个键返回一个列表的列表：所有示例的列表(长度为5)，然后是所有选项的列表(长度为4)以及输入id的列表(长度不同，因为我们没有应用任何填充):</p>
<pre><code class="lang-python">examples = datasets[<span class="hljs-string">"train"</span>][:<span class="hljs-number">5</span>]
features = preprocess_function(examples)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(features[<span class="hljs-string">"input_ids"</span>]), <span class="hljs-built_in">len</span>(features[<span class="hljs-string">"input_ids"</span>][<span class="hljs-number">0</span>]), [<span class="hljs-built_in">len</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> features[<span class="hljs-string">"input_ids"</span>][<span class="hljs-number">0</span>]])
</code></pre>
<pre><code>5 4 [30, 25, 30, 28]
</code></pre><p>让我们解码一下给定示例的输入:</p>
<pre><code class="lang-python">idx = <span class="hljs-number">3</span>
[tokenizer.decode(features[<span class="hljs-string">"input_ids"</span>][idx][i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)]
</code></pre>
<pre><code>['[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession are playing ping pong and celebrating one left each in quick. [SEP]',
 '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession wait slowly towards the cadets. [SEP]',
 '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions. [SEP]',
 '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession play and go back and forth hitting the drums while the audience claps for them. [SEP]']
</code></pre><p>我们可以将它和之前生成的ground truth进行比较：</p>
<pre><code class="lang-python">show_one(datasets[<span class="hljs-string">"train"</span>][<span class="hljs-number">3</span>])
</code></pre>
<pre><code>Context: A drum line passes by walking down the street playing their instruments.
  A - Members of the procession are playing ping pong and celebrating one left each in quick.
  B - Members of the procession wait slowly towards the cadets.
  C - Members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions.
  D - Members of the procession play and go back and forth hitting the drums while the audience claps for them.

Ground truth: option D
</code></pre><p>这似乎没问题。我们可以将这个函数应用到我们数据集的所有示例中，只需要使用我们之前创建的<code>dataset</code>对象的<code>map</code>方法。这将应用于<code>dataset</code>对象的所有切分的所有元素，所以我们的训练，验证和测试数据将以相同的方式进行预处理。</p>
<pre><code class="lang-python">encoded_datasets = datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
</code></pre>
<pre><code>Loading cached processed dataset at /home/sgugger/.cache/huggingface/datasets/swag/regular/0.0.0/f9784740e0964a3c799d68cec0d992cc267d3fe94f3e048175eca69d739b980d/cache-975c81cf12e5b7ac.arrow
Loading cached processed dataset at /home/sgugger/.cache/huggingface/datasets/swag/regular/0.0.0/f9784740e0964a3c799d68cec0d992cc267d3fe94f3e048175eca69d739b980d/cache-d4806d63f1eaf5cd.arrow
Loading cached processed dataset at /home/sgugger/.cache/huggingface/datasets/swag/regular/0.0.0/f9784740e0964a3c799d68cec0d992cc267d3fe94f3e048175eca69d739b980d/cache-258c9cd71b0182db.arrow
</code></pre><p>更好的是，结果会被🤗Datasets库自动缓存，以避免下次运行时在这一步上花费时间。🤗Datasets库通常足够智能，它可以检测传递给<code>map</code>的函数何时发生更改(此时不再使用缓存数据)。例如，它将检测您是否在第一个单元格中更改了任务并重新运行笔记本。当🤗Datasets使用缓存文件时，它提示相应的警告，你可以在调用<code>map</code>中传入<code>load_from_cache_file=False</code>从而不使用缓存文件，并强制进行预处理。</p>
<p>请注意，我们传递了<code>batched=True</code>以批量对文本进行编码。这是为了充分利用我们前面加载的快速tokenizer的优势，它将使用多线程并发地处理批中的文本。</p>
<h2 id="微调模型">微调模型</h2>
<p>现在我们的数据已经准备好了，我们可以下载预训练好的模型并对其进行微调。因为我们的任务是关于多项选择的，所以我们使用<code>AutoModelForMultipleChoice</code>类。与tokenizer一样，<code>from_pretrained</code>方法将为我们下载并缓存模型。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMultipleChoice, TrainingArguments, Trainer

model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)
</code></pre>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre><p>这个警告告诉我们，我们正在丢弃一些权重(<code>vocab_transform</code>和<code>vocab_layer_norm</code>层)，并随机初始化其他一些参数(<code>pre_classifier</code>和<code>classifier</code>层)。这是完全正常的情况，因为我们舍弃了在预训练模型时用于掩码语言建模的头，代之以一个新的多选头，并且我们没有其预训练好的权重，所以这个警告告诉我们使用这个模型来推理之前需要微调，而这正是我们要做的。</p>
<p>为了实例化一个<code>Trainer</code>，我们需要定义另外三个东西。最重要的是<a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments" target="_blank"><code>TrainingArguments</code></a>，它是一个包含所有用于训练的属性的类。它需要传入一个文件夹名，用于保存模型的检查点，而所有其他参数都是可选的:</p>
<pre><code class="lang-python">args = TrainingArguments(
    <span class="hljs-string">"test-glue"</span>,
    evaluation_strategy = <span class="hljs-string">"epoch"</span>,
    learning_rate=<span class="hljs-number">5e-5</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)
</code></pre>
<p>在这里，我们设置在每个epoch的末尾进行评估，调整学习速率，使用在jupyter笔记本顶部定义的<code>batch_size</code>，并定制用于训练的epoch的数量，以及权重衰减。</p>
<p>然后，我们需要告诉我们的<code>Trainer</code>如何从预处理的输入数据中构造批数据。我们还没有做任何填充，因为我们将填充每个批到批内的最大长度(而不是使用整个数据集的最大长度)。这将是<em>data collator</em>的工作。它接受示例的列表，并将它们转换为一个批(在我们的示例中，通过应用填充)。由于在库中没有data collator来处理我们的特定问题，这里我们根据<code>DataCollatorWithPadding</code>自行改编一个:</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass
<span class="hljs-keyword">from</span> transformers.tokenization_utils_base <span class="hljs-keyword">import</span> PreTrainedTokenizerBase, PaddingStrategy
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span>, <span class="hljs-type">Union</span>
<span class="hljs-keyword">import</span> torch

<span class="hljs-meta">@dataclass</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">DataCollatorForMultipleChoice</span>:
    <span class="hljs-string">"""
    Data collator that will dynamically pad the inputs for multiple choice received.
    """</span>

    tokenizer: PreTrainedTokenizerBase
    padding: <span class="hljs-type">Union</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">str</span>, PaddingStrategy] = <span class="hljs-literal">True</span>
    max_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>
    pad_to_multiple_of: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, features</span>):
        label_name = <span class="hljs-string">"label"</span> <span class="hljs-keyword">if</span> <span class="hljs-string">"label"</span> <span class="hljs-keyword">in</span> features[<span class="hljs-number">0</span>].keys() <span class="hljs-keyword">else</span> <span class="hljs-string">"labels"</span>
        labels = [feature.pop(label_name) <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]
        batch_size = <span class="hljs-built_in">len</span>(features)
        num_choices = <span class="hljs-built_in">len</span>(features[<span class="hljs-number">0</span>][<span class="hljs-string">"input_ids"</span>])
        flattened_features = [[{k: v[i] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> feature.items()} <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_choices)] <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]
        flattened_features = <span class="hljs-built_in">sum</span>(flattened_features, [])

        batch = <span class="hljs-variable language_">self</span>.tokenizer.pad(
            flattened_features,
            padding=<span class="hljs-variable language_">self</span>.padding,
            max_length=<span class="hljs-variable language_">self</span>.max_length,
            pad_to_multiple_of=<span class="hljs-variable language_">self</span>.pad_to_multiple_of,
            return_tensors=<span class="hljs-string">"pt"</span>,
        )

        <span class="hljs-comment"># Un-flatten</span>
        batch = {k: v.view(batch_size, num_choices, -<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
        <span class="hljs-comment"># Add back labels</span>
        batch[<span class="hljs-string">"labels"</span>] = torch.tensor(labels, dtype=torch.int64)
        <span class="hljs-keyword">return</span> batch
</code></pre>
<p>当传入一个示例的列表时，它会将大列表中的所有输入/注意力掩码等都压平，并传递给<code>tokenizer.pad</code>方法。这将返回一个带有大张量的字典(其大小为<code>(batch_size * 4) x seq_length</code>)，然后我们将其展开。</p>
<p>我们可以在特征列表上检查data collator是否正常工作，在这里，我们只需要确保删除所有不被我们的模型接受的输入特征(这是<code>Trainer</code>自动为我们做的)：</p>
<pre><code class="lang-python">accepted_keys = [<span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"label"</span>]
features = [{k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_datasets[<span class="hljs-string">"train"</span>][i].items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">in</span> accepted_keys} <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)]
batch = DataCollatorForMultipleChoice(tokenizer)(features)
</code></pre>
<p>再次强调，所有这些压平的、未压平的都可能是潜在错误的来源，所以让我们对输入进行另一个完整性检查：</p>
<pre><code class="lang-python">[tokenizer.decode(batch[<span class="hljs-string">"input_ids"</span>][<span class="hljs-number">8</span>][i].tolist()) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)]
</code></pre>
<pre><code>['[CLS] someone walks over to the radio. [SEP] someone hands her another phone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',
 '[CLS] someone walks over to the radio. [SEP] someone takes the drink, then holds it. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',
 '[CLS] someone walks over to the radio. [SEP] someone looks off then looks at someone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',
 '[CLS] someone walks over to the radio. [SEP] someone stares blearily down at the floor. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']
</code></pre><pre><code class="lang-python">show_one(datasets[<span class="hljs-string">"train"</span>][<span class="hljs-number">8</span>])
</code></pre>
<pre><code>Context: Someone walks over to the radio.
  A - Someone hands her another phone.
  B - Someone takes the drink, then holds it.
  C - Someone looks off then looks at someone.
  D - Someone stares blearily down at the floor.

Ground truth: option D
</code></pre><p>所有的都正常运行!</p>
<p>最后要为<code>Trainer</code>定义如何根据预测计算评估指标。我们需要来定义一个函数，它将使用我们之前加载的<code>metric</code>，我们必须做的唯一预处理是取我们预测的logits的argmax：</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_predictions</span>):
    predictions, label_ids = eval_predictions
    preds = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">"accuracy"</span>: (preds == label_ids).astype(np.float32).mean().item()}
</code></pre>
<p>然后，我们只需要将所有这些以及我们的数据集一起传入<code>Trainer</code>：</p>
<pre><code class="lang-python">trainer = Trainer(
    model,
    args,
    train_dataset=encoded_datasets[<span class="hljs-string">"train"</span>],
    eval_dataset=encoded_datasets[<span class="hljs-string">"validation"</span>],
    tokenizer=tokenizer,
    data_collator=DataCollatorForMultipleChoice(tokenizer),
    compute_metrics=compute_metrics,
)
</code></pre>
<p>现在，我们可以通过调用<code>train</code>方法来微调模型：</p>
<pre><code class="lang-python">trainer.train()
</code></pre>
<pre><code>&lt;div&gt;
    &lt;style&gt;
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
    &lt;/style&gt;

  &lt;progress value='6897' max='6897' style='width:300px; height:20px; vertical-align: middle;'&gt;&lt;/progress&gt;
  [6897/6897 23:49, Epoch 3/3]
&lt;/div&gt;
&lt;table border="1" class="dataframe"&gt;
</code></pre><p>  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.154598</td>
      <td>0.828017</td>
      <td>0.766520</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.296633</td>
      <td>0.667454</td>
      <td>0.786814</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.111786</td>
      <td>0.994927</td>
      <td>0.789363</td>
    </tr>
  </tbody>
&lt;/table&gt;</p><p></p>
<pre><code>TrainOutput(global_step=6897, training_loss=0.19714653808275168)
</code></pre><p>最后，不要忘记将你的模型<a href="https://huggingface.co/transformers/model_sharing.html" target="_blank">上传</a>到<a href="https://huggingface.co/models" target="_blank">🤗 模型中心</a>。</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="4.3-问答任务-抽取式问答.html" class="navigation navigation-prev " aria-label="Previous page: 4.3-问答任务-抽取式问答">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="4.5-生成任务-语言模型.html" class="navigation navigation-next " aria-label="Next page: 4.5-生成任务-语言模型">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"4.4-问答任务-多选问答","level":"4.6","depth":1,"next":{"title":"4.5-生成任务-语言模型","level":"4.7","depth":1,"path":"篇章4-使用Transformers解决NLP任务/4.5-生成任务-语言模型.md","ref":"./篇章4-使用Transformers解决NLP任务/4.5-生成任务-语言模型.md","articles":[]},"previous":{"title":"4.3-问答任务-抽取式问答","level":"4.5","depth":1,"path":"篇章4-使用Transformers解决NLP任务/4.3-问答任务-抽取式问答.md","ref":"./篇章4-使用Transformers解决NLP任务/4.3-问答任务-抽取式问答.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"篇章4-使用Transformers解决NLP任务/4.4-问答任务-多选问答.md","mtime":"2024-08-23T15:34:37.390Z","type":"markdown"},"gitbook":{"version":"5.1.4","time":"2024-08-23T15:50:04.957Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

