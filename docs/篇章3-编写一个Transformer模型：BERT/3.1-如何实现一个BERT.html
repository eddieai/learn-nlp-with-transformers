
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <title>3.1-如何实现一个BERT · HonKit</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="HonKit 5.1.4">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/@honkit/honkit-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, user-scalable=yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="3.2-如何应用一个BERT.html" />
    
    
    <link rel="prev" href="../篇章2-Transformer相关原理/2.5-篇章小测.html" />
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">篇章1-前言</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../篇章1-前言/1.0-本地阅读和代码运行环境配置.html">
            
                <a href="../篇章1-前言/1.0-本地阅读和代码运行环境配置.html">
            
                    
                    1.0-本地阅读和代码运行环境配置
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../篇章1-前言/1.1-Transformers在NLP中的兴起.html">
            
                <a href="../篇章1-前言/1.1-Transformers在NLP中的兴起.html">
            
                    
                    1.1-Transformers在NLP中的兴起
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">篇章2-Transformer相关原理</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../篇章2-Transformer相关原理/2.0-前言.html">
            
                <a href="../篇章2-Transformer相关原理/2.0-前言.html">
            
                    
                    2.0-前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../篇章2-Transformer相关原理/2.1-图解attention.html">
            
                <a href="../篇章2-Transformer相关原理/2.1-图解attention.html">
            
                    
                    2.1-图解attention
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../篇章2-Transformer相关原理/2.2-图解transformer.html">
            
                <a href="../篇章2-Transformer相关原理/2.2-图解transformer.html">
            
                    
                    2.2-图解transformer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="../篇章2-Transformer相关原理/2.2.1-Pytorch编写Transformer.html">
            
                <a href="../篇章2-Transformer相关原理/2.2.1-Pytorch编写Transformer.html">
            
                    
                    2.2.1-Pytorch编写Transformer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.5" data-path="../篇章2-Transformer相关原理/2.2.1-Pytorch编写Transformer-选读.md">
            
                <span>
            
                    
                    2.2.2-Pytorch编写Transformer-选读
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.6" data-path="../篇章2-Transformer相关原理/2.3-图解BERT.html">
            
                <a href="../篇章2-Transformer相关原理/2.3-图解BERT.html">
            
                    
                    2.3-图解BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.7" data-path="../篇章2-Transformer相关原理/2.4-图解GPT.html">
            
                <a href="../篇章2-Transformer相关原理/2.4-图解GPT.html">
            
                    
                    2.4-图解GPT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.8" data-path="../篇章2-Transformer相关原理/2.5-篇章小测.html">
            
                <a href="../篇章2-Transformer相关原理/2.5-篇章小测.html">
            
                    
                    2.5-篇章小测
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">篇章3-编写一个Transformer模型：BERT</li>
        
        
    
        <li class="chapter active" data-level="3.1" data-path="3.1-如何实现一个BERT.html">
            
                <a href="3.1-如何实现一个BERT.html">
            
                    
                    3.1-如何实现一个BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="3.2-如何应用一个BERT.html">
            
                <a href="3.2-如何应用一个BERT.html">
            
                    
                    3.2-如何应用一个BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="3.3-篇章小测.html">
            
                <a href="3.3-篇章小测.html">
            
                    
                    3.3-篇章小测
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">篇章4-使用Transformers解决NLP任务</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="../篇章4-使用Transformers解决NLP任务/4.0-前言.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.0-前言.html">
            
                    
                    4.0-前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="../篇章4-使用Transformers解决NLP任务/4.0-基于HuggingFace-Transformers的预训练模型微调.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.0-基于HuggingFace-Transformers的预训练模型微调.html">
            
                    
                    4.0-基于HuggingFace-Transformers的预训练模型微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="../篇章4-使用Transformers解决NLP任务/4.1-文本分类.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.1-文本分类.html">
            
                    
                    4.1-文本分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="../篇章4-使用Transformers解决NLP任务/4.2-序列标注.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.2-序列标注.html">
            
                    
                    4.2-序列标注
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="../篇章4-使用Transformers解决NLP任务/4.3-问答任务-抽取式问答.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.3-问答任务-抽取式问答.html">
            
                    
                    4.3-问答任务-抽取式问答
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.6" data-path="../篇章4-使用Transformers解决NLP任务/4.4-问答任务-多选问答.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.4-问答任务-多选问答.html">
            
                    
                    4.4-问答任务-多选问答
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.7" data-path="../篇章4-使用Transformers解决NLP任务/4.5-生成任务-语言模型.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.5-生成任务-语言模型.html">
            
                    
                    4.5-生成任务-语言模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.8" data-path="../篇章4-使用Transformers解决NLP任务/4.6-生成任务-机器翻译.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.6-生成任务-机器翻译.html">
            
                    
                    4.6-生成任务-机器翻译
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.9" data-path="../篇章4-使用Transformers解决NLP任务/4.7-生成任务-摘要生成.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.7-生成任务-摘要生成.html">
            
                    
                    4.7-生成任务-摘要生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10" data-path="../篇章4-使用Transformers解决NLP任务/4.8-篇章小测.html">
            
                <a href="../篇章4-使用Transformers解决NLP任务/4.8-篇章小测.html">
            
                    
                    4.8-篇章小测
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://github.com/honkit/honkit" target="blank" class="gitbook-link">
            Published with HonKit
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >3.1-如何实现一个BERT</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="前言">前言</h2>
<p>本文包含大量源码和讲解，通过段落和横线分割了各个模块，同时网站配备了侧边栏，帮助大家在各个小节中快速跳转，希望大家阅读完能对BERT有深刻的了解。同时建议通过pycharm、vscode等工具对bert源码进行单步调试，调试到对应的模块再对比看本章节的讲解。</p>
<p>涉及到的jupyter可以在<a href="https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AATransformer%E6%A8%A1%E5%9E%8B%EF%BC%9ABERT" target="_blank">代码库：篇章3-编写一个Transformer模型：BERT，下载</a></p>
<p>本篇章将基于H<a href="https://github.com/huggingface/transformers" target="_blank">HuggingFace/Transformers, 48.9k Star</a>进行学习。本章节的全部代码在<a href="https://github.com/huggingface/transformers/tree/master/src/transformers/models/bert" target="_blank">huggingface bert，注意由于版本更新较快，可能存在差别，请以4.4.2版本为准</a>HuggingFace 是一家总部位于纽约的聊天机器人初创服务商，很早就捕捉到 BERT 大潮流的信号并着手实现基于 pytorch 的 BERT 模型。这一项目最初名为 pytorch-pretrained-bert，在复现了原始效果的同时，提供了易用的方法以方便在这一强大模型的基础上进行各种玩耍和研究。</p>
<p>随着使用人数的增加，这一项目也发展成为一个较大的开源社区，合并了各种预训练语言模型以及增加了 Tensorflow 的实现，并且在 2019 年下半年改名为 Transformers。截止写文章时（2021 年 3 月 30 日）这一项目已经拥有 43k+ 的star，可以说 Transformers 已经成为事实上的 NLP 基本工具。</p>
<h2 id="本小节主要内容">本小节主要内容</h2>
<p><img src="pictures/3-6-bert.png" alt="图：BERT结构"></img> 图：BERT结构，来源IrEne: Interpretable Energy Prediction for Transformers</p>
<p>本文基于 Transformers 版本 4.4.2（2021 年 3 月 19 日发布）项目中，pytorch 版的 BERT 相关代码，从代码结构、具体实现与原理，以及使用的角度进行分析。
主要包含内容：</p>
<ol>
<li>BERT Tokenization 分词模型（BertTokenizer）</li>
<li>BERT Model 本体模型（BertModel）<ul>
<li>BertEmbeddings</li>
<li>BertEncoder<ul>
<li>BertLayer<ul>
<li>BertAttention</li>
<li>BertIntermediate</li>
<li>BertOutput</li>
</ul>
</li>
</ul>
</li>
<li>BertPooler</li>
</ul>
</li>
</ol>
<hr></hr>
<h2 id="1-tokenization分词-berttokenizer">1-Tokenization分词-BertTokenizer</h2>
<p>和BERT 有关的 Tokenizer 主要写在<a href="https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py" target="_blank"><code>models/bert/tokenization_bert.py</code></a>中。</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> unicodedata
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span>, <span class="hljs-type">Tuple</span>

<span class="hljs-keyword">from</span> transformers.tokenization_utils <span class="hljs-keyword">import</span> PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace
<span class="hljs-keyword">from</span> transformers.utils <span class="hljs-keyword">import</span> logging


logger = logging.get_logger(__name__)

VOCAB_FILES_NAMES = {<span class="hljs-string">"vocab_file"</span>: <span class="hljs-string">"vocab.txt"</span>}

PRETRAINED_VOCAB_FILES_MAP = {
    <span class="hljs-string">"vocab_file"</span>: {
        <span class="hljs-string">"bert-base-uncased"</span>: <span class="hljs-string">"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt"</span>,
    }
}

PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
    <span class="hljs-string">"bert-base-uncased"</span>: <span class="hljs-number">512</span>,
}

PRETRAINED_INIT_CONFIGURATION = {
    <span class="hljs-string">"bert-base-uncased"</span>: {<span class="hljs-string">"do_lower_case"</span>: <span class="hljs-literal">True</span>},
}


<span class="hljs-keyword">def</span> <span class="hljs-title function_">load_vocab</span>(<span class="hljs-params">vocab_file</span>):
    <span class="hljs-string">"""Loads a vocabulary file into a dictionary."""</span>
    vocab = collections.OrderedDict()
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(vocab_file, <span class="hljs-string">"r"</span>, encoding=<span class="hljs-string">"utf-8"</span>) <span class="hljs-keyword">as</span> reader:
        tokens = reader.readlines()
    <span class="hljs-keyword">for</span> index, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tokens):
        token = token.rstrip(<span class="hljs-string">"\n"</span>)
        vocab[token] = index
    <span class="hljs-keyword">return</span> vocab


<span class="hljs-keyword">def</span> <span class="hljs-title function_">whitespace_tokenize</span>(<span class="hljs-params">text</span>):
    <span class="hljs-string">"""Runs basic whitespace cleaning and splitting on a piece of text."""</span>
    text = text.strip()
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> text:
        <span class="hljs-keyword">return</span> []
    tokens = text.split()
    <span class="hljs-keyword">return</span> tokens


<span class="hljs-keyword">class</span> <span class="hljs-title class_">BertTokenizer</span>(<span class="hljs-title class_ inherited__">PreTrainedTokenizer</span>):

    vocab_files_names = VOCAB_FILES_NAMES
    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION
    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">
        self,
        vocab_file,
        do_lower_case=<span class="hljs-literal">True</span>,
        do_basic_tokenize=<span class="hljs-literal">True</span>,
        never_split=<span class="hljs-literal">None</span>,
        unk_token=<span class="hljs-string">"[UNK]"</span>,
        sep_token=<span class="hljs-string">"[SEP]"</span>,
        pad_token=<span class="hljs-string">"[PAD]"</span>,
        cls_token=<span class="hljs-string">"[CLS]"</span>,
        mask_token=<span class="hljs-string">"[MASK]"</span>,
        tokenize_chinese_chars=<span class="hljs-literal">True</span>,
        strip_accents=<span class="hljs-literal">None</span>,
        **kwargs
    </span>):
        <span class="hljs-built_in">super</span>().__init__(
            do_lower_case=do_lower_case,
            do_basic_tokenize=do_basic_tokenize,
            never_split=never_split,
            unk_token=unk_token,
            sep_token=sep_token,
            pad_token=pad_token,
            cls_token=cls_token,
            mask_token=mask_token,
            tokenize_chinese_chars=tokenize_chinese_chars,
            strip_accents=strip_accents,
            **kwargs,
        )

        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isfile(vocab_file):
            <span class="hljs-keyword">raise</span> ValueError(
                <span class="hljs-string">f"Can't find a vocabulary file at path '<span class="hljs-subst">{vocab_file}</span>'. To load the vocabulary from a Google pretrained "</span>
                <span class="hljs-string">"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"</span>
            )
        <span class="hljs-variable language_">self</span>.vocab = load_vocab(vocab_file)
        <span class="hljs-variable language_">self</span>.ids_to_tokens = collections.OrderedDict([(ids, tok) <span class="hljs-keyword">for</span> tok, ids <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.vocab.items()])
        <span class="hljs-variable language_">self</span>.do_basic_tokenize = do_basic_tokenize
        <span class="hljs-keyword">if</span> do_basic_tokenize:
            <span class="hljs-variable language_">self</span>.basic_tokenizer = BasicTokenizer(
                do_lower_case=do_lower_case,
                never_split=never_split,
                tokenize_chinese_chars=tokenize_chinese_chars,
                strip_accents=strip_accents,
            )
        <span class="hljs-variable language_">self</span>.wordpiece_tokenizer = WordpieceTokenizer(vocab=<span class="hljs-variable language_">self</span>.vocab, unk_token=<span class="hljs-variable language_">self</span>.unk_token)

<span class="hljs-meta">    @property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_lower_case</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.basic_tokenizer.do_lower_case

<span class="hljs-meta">    @property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">vocab_size</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.vocab)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_vocab</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-built_in">dict</span>(<span class="hljs-variable language_">self</span>.vocab, **<span class="hljs-variable language_">self</span>.added_tokens_encoder)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_tokenize</span>(<span class="hljs-params">self, text</span>):
        split_tokens = []
        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.do_basic_tokenize:
            <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.basic_tokenizer.tokenize(text, never_split=<span class="hljs-variable language_">self</span>.all_special_tokens):

                <span class="hljs-comment"># If the token is part of the never_split set</span>
                <span class="hljs-keyword">if</span> token <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.basic_tokenizer.never_split:
                    split_tokens.append(token)
                <span class="hljs-keyword">else</span>:
                    split_tokens += <span class="hljs-variable language_">self</span>.wordpiece_tokenizer.tokenize(token)
        <span class="hljs-keyword">else</span>:
            split_tokens = <span class="hljs-variable language_">self</span>.wordpiece_tokenizer.tokenize(text)
        <span class="hljs-keyword">return</span> split_tokens

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_convert_token_to_id</span>(<span class="hljs-params">self, token</span>):
        <span class="hljs-string">"""Converts a token (str) in an id using the vocab."""</span>
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.vocab.get(token, <span class="hljs-variable language_">self</span>.vocab.get(<span class="hljs-variable language_">self</span>.unk_token))

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_convert_id_to_token</span>(<span class="hljs-params">self, index</span>):
        <span class="hljs-string">"""Converts an index (integer) in a token (str) using the vocab."""</span>
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.ids_to_tokens.get(index, <span class="hljs-variable language_">self</span>.unk_token)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_tokens_to_string</span>(<span class="hljs-params">self, tokens</span>):
        <span class="hljs-string">"""Converts a sequence of tokens (string) in a single string."""</span>
        out_string = <span class="hljs-string">" "</span>.join(tokens).replace(<span class="hljs-string">" ##"</span>, <span class="hljs-string">""</span>).strip()
        <span class="hljs-keyword">return</span> out_string

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_inputs_with_special_tokens</span>(<span class="hljs-params">
        self, token_ids_0: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], token_ids_1: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]] = <span class="hljs-literal">None</span>
    </span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:
        <span class="hljs-string">"""
        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
        adding special tokens. A BERT sequence has the following format:
        - single sequence: ``[CLS] X [SEP]``
        - pair of sequences: ``[CLS] A [SEP] B [SEP]``
        Args:
            token_ids_0 (:obj:`List[int]`):
                List of IDs to which the special tokens will be added.
            token_ids_1 (:obj:`List[int]`, `optional`):
                Optional second list of IDs for sequence pairs.
        Returns:
            :obj:`List[int]`: List of `input IDs &lt;../glossary.html#input-ids&gt;`__ with the appropriate special tokens.
        """</span>
        <span class="hljs-keyword">if</span> token_ids_1 <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">return</span> [<span class="hljs-variable language_">self</span>.cls_token_id] + token_ids_0 + [<span class="hljs-variable language_">self</span>.sep_token_id]
        cls = [<span class="hljs-variable language_">self</span>.cls_token_id]
        sep = [<span class="hljs-variable language_">self</span>.sep_token_id]
        <span class="hljs-keyword">return</span> cls + token_ids_0 + sep + token_ids_1 + sep

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_special_tokens_mask</span>(<span class="hljs-params">
        self, token_ids_0: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], token_ids_1: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]] = <span class="hljs-literal">None</span>, already_has_special_tokens: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>
    </span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:
        <span class="hljs-string">"""
        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
        special tokens using the tokenizer ``prepare_for_model`` method.
        Args:
            token_ids_0 (:obj:`List[int]`):
                List of IDs.
            token_ids_1 (:obj:`List[int]`, `optional`):
                Optional second list of IDs for sequence pairs.
            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):
                Whether or not the token list is already formatted with special tokens for the model.
        Returns:
            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.
        """</span>

        <span class="hljs-keyword">if</span> already_has_special_tokens:
            <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>().get_special_tokens_mask(
                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=<span class="hljs-literal">True</span>
            )

        <span class="hljs-keyword">if</span> token_ids_1 <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">return</span> [<span class="hljs-number">1</span>] + ([<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(token_ids_0)) + [<span class="hljs-number">1</span>] + ([<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(token_ids_1)) + [<span class="hljs-number">1</span>]
        <span class="hljs-keyword">return</span> [<span class="hljs-number">1</span>] + ([<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(token_ids_0)) + [<span class="hljs-number">1</span>]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">create_token_type_ids_from_sequences</span>(<span class="hljs-params">
        self, token_ids_0: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], token_ids_1: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]] = <span class="hljs-literal">None</span>
    </span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:
        <span class="hljs-string">"""
        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence
        pair mask has the following format:
        ::
            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
            | first sequence    | second sequence |
        If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).
        Args:
            token_ids_0 (:obj:`List[int]`):
                List of IDs.
            token_ids_1 (:obj:`List[int]`, `optional`):
                Optional second list of IDs for sequence pairs.
        Returns:
            :obj:`List[int]`: List of `token type IDs &lt;../glossary.html#token-type-ids&gt;`_ according to the given
            sequence(s).
        """</span>
        sep = [<span class="hljs-variable language_">self</span>.sep_token_id]
        cls = [<span class="hljs-variable language_">self</span>.cls_token_id]
        <span class="hljs-keyword">if</span> token_ids_1 <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(cls + token_ids_0 + sep) * [<span class="hljs-number">0</span>]
        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(cls + token_ids_0 + sep) * [<span class="hljs-number">0</span>] + <span class="hljs-built_in">len</span>(token_ids_1 + sep) * [<span class="hljs-number">1</span>]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_vocabulary</span>(<span class="hljs-params">self, save_directory: <span class="hljs-built_in">str</span>, filename_prefix: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">str</span>]:
        index = <span class="hljs-number">0</span>
        <span class="hljs-keyword">if</span> os.path.isdir(save_directory):
            vocab_file = os.path.join(
                save_directory, (filename_prefix + <span class="hljs-string">"-"</span> <span class="hljs-keyword">if</span> filename_prefix <span class="hljs-keyword">else</span> <span class="hljs-string">""</span>) + VOCAB_FILES_NAMES[<span class="hljs-string">"vocab_file"</span>]
            )
        <span class="hljs-keyword">else</span>:
            vocab_file = (filename_prefix + <span class="hljs-string">"-"</span> <span class="hljs-keyword">if</span> filename_prefix <span class="hljs-keyword">else</span> <span class="hljs-string">""</span>) + save_directory
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(vocab_file, <span class="hljs-string">"w"</span>, encoding=<span class="hljs-string">"utf-8"</span>) <span class="hljs-keyword">as</span> writer:
            <span class="hljs-keyword">for</span> token, token_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(<span class="hljs-variable language_">self</span>.vocab.items(), key=<span class="hljs-keyword">lambda</span> kv: kv[<span class="hljs-number">1</span>]):
                <span class="hljs-keyword">if</span> index != token_index:
                    logger.warning(
                        <span class="hljs-string">f"Saving vocabulary to <span class="hljs-subst">{vocab_file}</span>: vocabulary indices are not consecutive."</span>
                        <span class="hljs-string">" Please check that the vocabulary is not corrupted!"</span>
                    )
                    index = token_index
                writer.write(token + <span class="hljs-string">"\n"</span>)
                index += <span class="hljs-number">1</span>
        <span class="hljs-keyword">return</span> (vocab_file,)


<span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicTokenizer</span>(<span class="hljs-title class_ inherited__">object</span>):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, do_lower_case=<span class="hljs-literal">True</span>, never_split=<span class="hljs-literal">None</span>, tokenize_chinese_chars=<span class="hljs-literal">True</span>, strip_accents=<span class="hljs-literal">None</span></span>):
        <span class="hljs-keyword">if</span> never_split <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            never_split = []
        <span class="hljs-variable language_">self</span>.do_lower_case = do_lower_case
        <span class="hljs-variable language_">self</span>.never_split = <span class="hljs-built_in">set</span>(never_split)
        <span class="hljs-variable language_">self</span>.tokenize_chinese_chars = tokenize_chinese_chars
        <span class="hljs-variable language_">self</span>.strip_accents = strip_accents

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">self, text, never_split=<span class="hljs-literal">None</span></span>):
        <span class="hljs-string">"""
        Basic Tokenization of a piece of text. Split on "white spaces" only, for sub-word tokenization, see
        WordPieceTokenizer.
        Args:
            **never_split**: (`optional`) list of str
                Kept for backward compatibility purposes. Now implemented directly at the base class level (see
                :func:`PreTrainedTokenizer.tokenize`) List of token not to split.
        """</span>
        <span class="hljs-comment"># union() returns a new set by concatenating the two sets.</span>
        never_split = <span class="hljs-variable language_">self</span>.never_split.union(<span class="hljs-built_in">set</span>(never_split)) <span class="hljs-keyword">if</span> never_split <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.never_split
        text = <span class="hljs-variable language_">self</span>._clean_text(text)

        <span class="hljs-comment"># This was added on November 1st, 2018 for the multilingual and Chinese</span>
        <span class="hljs-comment"># models. This is also applied to the English models now, but it doesn't</span>
        <span class="hljs-comment"># matter since the English models were not trained on any Chinese data</span>
        <span class="hljs-comment"># and generally don't have any Chinese data in them (there are Chinese</span>
        <span class="hljs-comment"># characters in the vocabulary because Wikipedia does have some Chinese</span>
        <span class="hljs-comment"># words in the English Wikipedia.).</span>
        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.tokenize_chinese_chars:
            text = <span class="hljs-variable language_">self</span>._tokenize_chinese_chars(text)
        orig_tokens = whitespace_tokenize(text)
        split_tokens = []
        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> orig_tokens:
            <span class="hljs-keyword">if</span> token <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> never_split:
                <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.do_lower_case:
                    token = token.lower()
                    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.strip_accents <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">False</span>:
                        token = <span class="hljs-variable language_">self</span>._run_strip_accents(token)
                <span class="hljs-keyword">elif</span> <span class="hljs-variable language_">self</span>.strip_accents:
                    token = <span class="hljs-variable language_">self</span>._run_strip_accents(token)
            split_tokens.extend(<span class="hljs-variable language_">self</span>._run_split_on_punc(token, never_split))

        output_tokens = whitespace_tokenize(<span class="hljs-string">" "</span>.join(split_tokens))
        <span class="hljs-keyword">return</span> output_tokens

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_run_strip_accents</span>(<span class="hljs-params">self, text</span>):
        <span class="hljs-string">"""Strips accents from a piece of text."""</span>
        text = unicodedata.normalize(<span class="hljs-string">"NFD"</span>, text)
        output = []
        <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> text:
            cat = unicodedata.category(char)
            <span class="hljs-keyword">if</span> cat == <span class="hljs-string">"Mn"</span>:
                <span class="hljs-keyword">continue</span>
            output.append(char)
        <span class="hljs-keyword">return</span> <span class="hljs-string">""</span>.join(output)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_run_split_on_punc</span>(<span class="hljs-params">self, text, never_split=<span class="hljs-literal">None</span></span>):
        <span class="hljs-string">"""Splits punctuation on a piece of text."""</span>
        <span class="hljs-keyword">if</span> never_split <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> text <span class="hljs-keyword">in</span> never_split:
            <span class="hljs-keyword">return</span> [text]
        chars = <span class="hljs-built_in">list</span>(text)
        i = <span class="hljs-number">0</span>
        start_new_word = <span class="hljs-literal">True</span>
        output = []
        <span class="hljs-keyword">while</span> i &lt; <span class="hljs-built_in">len</span>(chars):
            char = chars[i]
            <span class="hljs-keyword">if</span> _is_punctuation(char):
                output.append([char])
                start_new_word = <span class="hljs-literal">True</span>
            <span class="hljs-keyword">else</span>:
                <span class="hljs-keyword">if</span> start_new_word:
                    output.append([])
                start_new_word = <span class="hljs-literal">False</span>
                output[-<span class="hljs-number">1</span>].append(char)
            i += <span class="hljs-number">1</span>

        <span class="hljs-keyword">return</span> [<span class="hljs-string">""</span>.join(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> output]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_tokenize_chinese_chars</span>(<span class="hljs-params">self, text</span>):
        <span class="hljs-string">"""Adds whitespace around any CJK character."""</span>
        output = []
        <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> text:
            cp = <span class="hljs-built_in">ord</span>(char)
            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>._is_chinese_char(cp):
                output.append(<span class="hljs-string">" "</span>)
                output.append(char)
                output.append(<span class="hljs-string">" "</span>)
            <span class="hljs-keyword">else</span>:
                output.append(char)
        <span class="hljs-keyword">return</span> <span class="hljs-string">""</span>.join(output)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_is_chinese_char</span>(<span class="hljs-params">self, cp</span>):
        <span class="hljs-string">"""Checks whether CP is the codepoint of a CJK character."""</span>
        <span class="hljs-comment"># This defines a "chinese character" as anything in the CJK Unicode block:</span>
        <span class="hljs-comment">#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)</span>
        <span class="hljs-comment">#</span>
        <span class="hljs-comment"># Note that the CJK Unicode block is NOT all Japanese and Korean characters,</span>
        <span class="hljs-comment"># despite its name. The modern Korean Hangul alphabet is a different block,</span>
        <span class="hljs-comment"># as is Japanese Hiragana and Katakana. Those alphabets are used to write</span>
        <span class="hljs-comment"># space-separated words, so they are not treated specially and handled</span>
        <span class="hljs-comment"># like the all of the other languages.</span>
        <span class="hljs-keyword">if</span> (
            (cp &gt;= <span class="hljs-number">0x4E00</span> <span class="hljs-keyword">and</span> cp &lt;= <span class="hljs-number">0x9FFF</span>)
            <span class="hljs-keyword">or</span> (cp &gt;= <span class="hljs-number">0x3400</span> <span class="hljs-keyword">and</span> cp &lt;= <span class="hljs-number">0x4DBF</span>)  <span class="hljs-comment">#</span>
            <span class="hljs-keyword">or</span> (cp &gt;= <span class="hljs-number">0x20000</span> <span class="hljs-keyword">and</span> cp &lt;= <span class="hljs-number">0x2A6DF</span>)  <span class="hljs-comment">#</span>
            <span class="hljs-keyword">or</span> (cp &gt;= <span class="hljs-number">0x2A700</span> <span class="hljs-keyword">and</span> cp &lt;= <span class="hljs-number">0x2B73F</span>)  <span class="hljs-comment">#</span>
            <span class="hljs-keyword">or</span> (cp &gt;= <span class="hljs-number">0x2B740</span> <span class="hljs-keyword">and</span> cp &lt;= <span class="hljs-number">0x2B81F</span>)  <span class="hljs-comment">#</span>
            <span class="hljs-keyword">or</span> (cp &gt;= <span class="hljs-number">0x2B820</span> <span class="hljs-keyword">and</span> cp &lt;= <span class="hljs-number">0x2CEAF</span>)  <span class="hljs-comment">#</span>
            <span class="hljs-keyword">or</span> (cp &gt;= <span class="hljs-number">0xF900</span> <span class="hljs-keyword">and</span> cp &lt;= <span class="hljs-number">0xFAFF</span>)
            <span class="hljs-keyword">or</span> (cp &gt;= <span class="hljs-number">0x2F800</span> <span class="hljs-keyword">and</span> cp &lt;= <span class="hljs-number">0x2FA1F</span>)  <span class="hljs-comment">#</span>
        ):  <span class="hljs-comment">#</span>
            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>

        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_clean_text</span>(<span class="hljs-params">self, text</span>):
        <span class="hljs-string">"""Performs invalid character removal and whitespace cleanup on text."""</span>
        output = []
        <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> text:
            cp = <span class="hljs-built_in">ord</span>(char)
            <span class="hljs-keyword">if</span> cp == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> cp == <span class="hljs-number">0xFFFD</span> <span class="hljs-keyword">or</span> _is_control(char):
                <span class="hljs-keyword">continue</span>
            <span class="hljs-keyword">if</span> _is_whitespace(char):
                output.append(<span class="hljs-string">" "</span>)
            <span class="hljs-keyword">else</span>:
                output.append(char)
        <span class="hljs-keyword">return</span> <span class="hljs-string">""</span>.join(output)


<span class="hljs-keyword">class</span> <span class="hljs-title class_">WordpieceTokenizer</span>(<span class="hljs-title class_ inherited__">object</span>):
    <span class="hljs-string">"""Runs WordPiece tokenization."""</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab, unk_token, max_input_chars_per_word=<span class="hljs-number">100</span></span>):
        <span class="hljs-variable language_">self</span>.vocab = vocab
        <span class="hljs-variable language_">self</span>.unk_token = unk_token
        <span class="hljs-variable language_">self</span>.max_input_chars_per_word = max_input_chars_per_word

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">self, text</span>):
        <span class="hljs-string">"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.
        For example, :obj:`input = "unaffable"` wil return as output :obj:`["un", "##aff", "##able"]`.
        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.
        Returns:
          A list of wordpiece tokens.
        """</span>

        output_tokens = []
        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> whitespace_tokenize(text):
            chars = <span class="hljs-built_in">list</span>(token)
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(chars) &gt; <span class="hljs-variable language_">self</span>.max_input_chars_per_word:
                output_tokens.append(<span class="hljs-variable language_">self</span>.unk_token)
                <span class="hljs-keyword">continue</span>

            is_bad = <span class="hljs-literal">False</span>
            start = <span class="hljs-number">0</span>
            sub_tokens = []
            <span class="hljs-keyword">while</span> start &lt; <span class="hljs-built_in">len</span>(chars):
                end = <span class="hljs-built_in">len</span>(chars)
                cur_substr = <span class="hljs-literal">None</span>
                <span class="hljs-keyword">while</span> start &lt; end:
                    substr = <span class="hljs-string">""</span>.join(chars[start:end])
                    <span class="hljs-keyword">if</span> start &gt; <span class="hljs-number">0</span>:
                        substr = <span class="hljs-string">"##"</span> + substr
                    <span class="hljs-keyword">if</span> substr <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.vocab:
                        cur_substr = substr
                        <span class="hljs-keyword">break</span>
                    end -= <span class="hljs-number">1</span>
                <span class="hljs-keyword">if</span> cur_substr <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
                    is_bad = <span class="hljs-literal">True</span>
                    <span class="hljs-keyword">break</span>
                sub_tokens.append(cur_substr)
                start = end

            <span class="hljs-keyword">if</span> is_bad:
                output_tokens.append(<span class="hljs-variable language_">self</span>.unk_token)
            <span class="hljs-keyword">else</span>:
                output_tokens.extend(sub_tokens)
        <span class="hljs-keyword">return</span> output_tokens
</code></pre>
<pre><code>class BertTokenizer(PreTrainedTokenizer):
    """
    Construct a BERT tokenizer. Based on WordPiece.

    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.
    Users should refer to this superclass for more information regarding those methods.
    ...
    """
</code></pre><p><code>BertTokenizer</code> 是基于<code>BasicTokenizer</code>和<code>WordPieceTokenizer</code>的分词器：</p>
<ul>
<li>BasicTokenizer负责处理的第一步——按标点、空格等分割句子，并处理是否统一小写，以及清理非法字符。<ul>
<li>对于中文字符，通过预处理（加空格）来按字分割；</li>
<li>同时可以通过never_split指定对某些词不进行分割；</li>
<li>这一步是可选的（默认执行）。</li>
</ul>
</li>
<li>WordPieceTokenizer在词的基础上，进一步将词分解为子词（subword）。<ul>
<li>subword 介于 char 和 word 之间，既在一定程度保留了词的含义，又能够照顾到英文中单复数、时态导致的词表爆炸和未登录词的 OOV（Out-Of-Vocabulary）问题，将词根与时态词缀等分割出来，从而减小词表，也降低了训练难度；</li>
<li>例如，tokenizer 这个词就可以拆解为“token”和“##izer”两部分，注意后面一个词的“##”表示接在前一个词后面。
BertTokenizer 有以下常用方法：</li>
</ul>
</li>
<li>from_pretrained：从包含词表文件（vocab.txt）的目录中初始化一个分词器；</li>
<li>tokenize：将文本（词或者句子）分解为子词列表；</li>
<li>convert_tokens_to_ids：将子词列表转化为子词对应下标的列表；</li>
<li>convert_ids_to_tokens ：与上一个相反；</li>
<li>convert_tokens_to_string：将 subword 列表按“##”拼接回词或者句子；</li>
<li>encode：对于单个句子输入，分解词并加入特殊词形成“[CLS], x, [SEP]”的结构并转换为词表对应下标的列表；对于两个句子输入（多个句子只取前两个），分解词并加入特殊词形成“[CLS], x1, [SEP], x2, [SEP]”的结构并转换为下标列表；</li>
<li>decode：可以将 encode 方法的输出变为完整句子。
以及，类自身的方法：</li>
</ul>
<pre><code class="lang-python">bt = BertTokenizer.from_pretrained(<span class="hljs-string">'bert-base-uncased'</span>)
bt(<span class="hljs-string">'I like natural language progressing!'</span>)
<span class="hljs-comment"># {'input_ids': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}</span>
</code></pre>
<pre><code>Downloading: 100%|██████████| 232k/232k [00:00&lt;00:00, 698kB/s]
Downloading: 100%|██████████| 28.0/28.0 [00:00&lt;00:00, 11.1kB/s]
Downloading: 100%|██████████| 466k/466k [00:00&lt;00:00, 863kB/s]





{'input_ids': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre><hr></hr>
<h2 id="2-model-bertmodel">2-Model-BertModel</h2>
<p>和 BERT 模型有关的代码主要写在<a href="https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py" target="_blank"><code>/models/bert/modeling_bert.py</code></a>中，这一份代码有一千多行，包含 BERT 模型的基本结构和基于它的微调模型等。</p>
<p>下面从 BERT 模型本体入手分析：</p>
<pre><code>class BertModel(BertPreTrainedModel):
    """

    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
    cross-attention is added between the self-attention layers, following the architecture described in `Attention is
    all you need &lt;https://arxiv.org/abs/1706.03762&gt;`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.

    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration
    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`
    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an
    input to the forward pass.
    """
</code></pre><p>BertModel 主要为 transformer encoder 结构，包含三个部分：</p>
<ol>
<li>embeddings，即BertEmbeddings类的实体，根据单词符号获取对应的向量表示；</li>
<li>encoder，即BertEncoder类的实体；</li>
<li>pooler，即BertPooler类的实体，这一部分是可选的。</li>
</ol>
<p><strong>注意 BertModel 也可以配置为 Decoder，不过下文中不包含对这一部分的讨论。</strong></p>
<p>下面将介绍 BertModel 的前向传播过程中各个参数的含义以及返回值：</p>
<pre><code>def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_values=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ): ...
</code></pre><ul>
<li>input_ids：经过 tokenizer 分词后的 subword 对应的下标列表；</li>
<li>attention_mask：在 self-attention 过程中，这一块 mask 用于标记 subword 所处句子和 padding 的区别，将 padding 部分填充为 0；</li>
<li>token_type_ids：标记 subword 当前所处句子（第一句/第二句/ padding）；</li>
<li>position_ids：标记当前词所在句子的位置下标；</li>
<li>head_mask：用于将某些层的某些注意力计算无效化；</li>
<li>inputs_embeds：如果提供了，那就不需要input_ids，跨过 embedding lookup 过程直接作为 Embedding 进入 Encoder 计算；</li>
<li>encoder_hidden_states：这一部分在 BertModel 配置为 decoder 时起作用，将执行 cross-attention 而不是 self-attention；</li>
<li>encoder_attention_mask：同上，在 cross-attention 中用于标记 encoder 端输入的 padding；</li>
<li>past_key_values：这个参数貌似是把预先计算好的 K-V 乘积传入，以降低 cross-attention 的开销（因为原本这部分是重复计算）；</li>
<li>use_cache：将保存上一个参数并传回，加速 decoding；</li>
<li>output_attentions：是否返回中间每层的 attention 输出；</li>
<li>output_hidden_states：是否返回中间每层的输出；</li>
<li>return_dict：是否按键值对的形式（ModelOutput 类，也可以当作 tuple 用）返回输出，默认为真。</li>
</ul>
<p><strong>注意，这里的 head_mask 对注意力计算的无效化，和下文提到的注意力头剪枝不同，而仅仅把某些注意力的计算结果给乘以这一系数。</strong></p>
<p>输出部分如下：</p>
<pre><code># BertModel的前向传播返回部分
        if not return_dict:
            return (sequence_output, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPoolingAndCrossAttentions(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            past_key_values=encoder_outputs.past_key_values,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
            cross_attentions=encoder_outputs.cross_attentions,
        )
</code></pre><p>可以看出，返回值不但包含了 encoder 和 pooler 的输出，也包含了其他指定输出的部分（hidden_states 和 attention 等，这一部分在encoder_outputs[1:]）方便取用：</p>
<pre><code>        # BertEncoder的前向传播返回部分，即上面的encoder_outputs
        if not return_dict:
            return tuple(
                v
                for v in [
                    hidden_states,
                    next_decoder_cache,
                    all_hidden_states,
                    all_self_attentions,
                    all_cross_attentions,
                ]
                if v is not None
            )
        return BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=hidden_states,
            past_key_values=next_decoder_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
            cross_attentions=all_cross_attentions,
        )
</code></pre><p>此外，BertModel 还有以下的方法，方便 BERT 玩家进行各种操作：</p>
<ul>
<li>get_input_embeddings：提取 embedding 中的 word_embeddings 即词向量部分；</li>
<li>set_input_embeddings：为 embedding 中的 word_embeddings 赋值；</li>
<li>_prune_heads：提供了将注意力头剪枝的函数，输入为{layer_num: list of heads to prune in this layer}的字典，可以将指定层的某些注意力头剪枝。</li>
</ul>
<p><strong> 剪枝是一个复杂的操作，需要将保留的注意力头部分的 Wq、Kq、Vq 和拼接后全连接部分的权重拷贝到一个新的较小的权重矩阵（注意先禁止 grad 再拷贝），并实时记录被剪掉的头以防下标出错。具体参考BertAttention部分的prune_heads方法.</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers.models.bert.modeling_bert <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">class</span> <span class="hljs-title class_">BertModel</span>(<span class="hljs-title class_ inherited__">BertPreTrainedModel</span>):
    <span class="hljs-string">"""
    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
    cross-attention is added between the self-attention layers, following the architecture described in `Attention is
    all you need &lt;https://arxiv.org/abs/1706.03762&gt;`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.
    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration
    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`
    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an
    input to the forward pass.
    """</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, add_pooling_layer=<span class="hljs-literal">True</span></span>):
        <span class="hljs-built_in">super</span>().__init__(config)
        <span class="hljs-variable language_">self</span>.config = config

        <span class="hljs-variable language_">self</span>.embeddings = BertEmbeddings(config)
        <span class="hljs-variable language_">self</span>.encoder = BertEncoder(config)

        <span class="hljs-variable language_">self</span>.pooler = BertPooler(config) <span class="hljs-keyword">if</span> add_pooling_layer <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>

        <span class="hljs-variable language_">self</span>.init_weights()

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_input_embeddings</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.embeddings.word_embeddings

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_input_embeddings</span>(<span class="hljs-params">self, value</span>):
        <span class="hljs-variable language_">self</span>.embeddings.word_embeddings = value

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_prune_heads</span>(<span class="hljs-params">self, heads_to_prune</span>):
        <span class="hljs-string">"""
        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base
        class PreTrainedModel
        """</span>
        <span class="hljs-keyword">for</span> layer, heads <span class="hljs-keyword">in</span> heads_to_prune.items():
            <span class="hljs-variable language_">self</span>.encoder.layer[layer].attention.prune_heads(heads)

<span class="hljs-meta">    @add_start_docstrings_to_model_forward(<span class="hljs-params">BERT_INPUTS_DOCSTRING.<span class="hljs-built_in">format</span>(<span class="hljs-params"><span class="hljs-string">"batch_size, sequence_length"</span></span>)</span>)</span>
<span class="hljs-meta">    @add_code_sample_docstrings(<span class="hljs-params">
        tokenizer_class=_TOKENIZER_FOR_DOC,
        checkpoint=_CHECKPOINT_FOR_DOC,
        output_type=BaseModelOutputWithPoolingAndCrossAttentions,
        config_class=_CONFIG_FOR_DOC,
    </span>)</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">
        self,
        input_ids=<span class="hljs-literal">None</span>,
        attention_mask=<span class="hljs-literal">None</span>,
        token_type_ids=<span class="hljs-literal">None</span>,
        position_ids=<span class="hljs-literal">None</span>,
        head_mask=<span class="hljs-literal">None</span>,
        inputs_embeds=<span class="hljs-literal">None</span>,
        encoder_hidden_states=<span class="hljs-literal">None</span>,
        encoder_attention_mask=<span class="hljs-literal">None</span>,
        past_key_values=<span class="hljs-literal">None</span>,
        use_cache=<span class="hljs-literal">None</span>,
        output_attentions=<span class="hljs-literal">None</span>,
        output_hidden_states=<span class="hljs-literal">None</span>,
        return_dict=<span class="hljs-literal">None</span>,
    </span>):
        <span class="hljs-string">r"""
        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):
            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
            the model is configured as a decoder.
        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:
            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.
        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):
            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`
            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`
            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.
        use_cache (:obj:`bool`, `optional`):
            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up
            decoding (see :obj:`past_key_values`).
        """</span>
        output_attentions = output_attentions <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.config.output_attentions
        output_hidden_states = (
            output_hidden_states <span class="hljs-keyword">if</span> output_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.config.output_hidden_states
        )
        return_dict = return_dict <span class="hljs-keyword">if</span> return_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.config.use_return_dict

        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.config.is_decoder:
            use_cache = use_cache <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.config.use_cache
        <span class="hljs-keyword">else</span>:
            use_cache = <span class="hljs-literal">False</span>

        <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"You cannot specify both input_ids and inputs_embeds at the same time"</span>)
        <span class="hljs-keyword">elif</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            input_shape = input_ids.size()
            batch_size, seq_length = input_shape
        <span class="hljs-keyword">elif</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]
            batch_size, seq_length = input_shape
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"You have to specify either input_ids or inputs_embeds"</span>)

        device = input_ids.device <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> inputs_embeds.device

        <span class="hljs-comment"># past_key_values_length</span>
        past_key_values_length = past_key_values[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape[<span class="hljs-number">2</span>] <span class="hljs-keyword">if</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>

        <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)

        <span class="hljs-keyword">if</span> token_type_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(<span class="hljs-variable language_">self</span>.embeddings, <span class="hljs-string">"token_type_ids"</span>):
                buffered_token_type_ids = <span class="hljs-variable language_">self</span>.embeddings.token_type_ids[:, :seq_length]
                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)
                token_type_ids = buffered_token_type_ids_expanded
            <span class="hljs-keyword">else</span>:
                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)

        <span class="hljs-comment"># We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]</span>
        <span class="hljs-comment"># ourselves in which case we just need to make it broadcastable to all heads.</span>
        extended_attention_mask: torch.Tensor = <span class="hljs-variable language_">self</span>.get_extended_attention_mask(attention_mask, input_shape, device)

        <span class="hljs-comment"># If a 2D or 3D attention mask is provided for the cross-attention</span>
        <span class="hljs-comment"># we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]</span>
        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.config.is_decoder <span class="hljs-keyword">and</span> encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
            <span class="hljs-keyword">if</span> encoder_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
            encoder_extended_attention_mask = <span class="hljs-variable language_">self</span>.invert_attention_mask(encoder_attention_mask)
        <span class="hljs-keyword">else</span>:
            encoder_extended_attention_mask = <span class="hljs-literal">None</span>

        <span class="hljs-comment"># Prepare head mask if needed</span>
        <span class="hljs-comment"># 1.0 in head_mask indicate we keep the head</span>
        <span class="hljs-comment"># attention_probs has shape bsz x n_heads x N x N</span>
        <span class="hljs-comment"># input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span>
        <span class="hljs-comment"># and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]</span>
        head_mask = <span class="hljs-variable language_">self</span>.get_head_mask(head_mask, <span class="hljs-variable language_">self</span>.config.num_hidden_layers)

        embedding_output = <span class="hljs-variable language_">self</span>.embeddings(
            input_ids=input_ids,
            position_ids=position_ids,
            token_type_ids=token_type_ids,
            inputs_embeds=inputs_embeds,
            past_key_values_length=past_key_values_length,
        )
        encoder_outputs = <span class="hljs-variable language_">self</span>.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        sequence_output = encoder_outputs[<span class="hljs-number">0</span>]
        pooled_output = <span class="hljs-variable language_">self</span>.pooler(sequence_output) <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.pooler <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>

        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:
            <span class="hljs-keyword">return</span> (sequence_output, pooled_output) + encoder_outputs[<span class="hljs-number">1</span>:]

        <span class="hljs-keyword">return</span> BaseModelOutputWithPoolingAndCrossAttentions(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            past_key_values=encoder_outputs.past_key_values,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
            cross_attentions=encoder_outputs.cross_attentions,
        )
</code></pre>
<hr></hr>
<h3 id="21-bertembeddings">2.1-BertEmbeddings</h3>
<p>包含三个部分求和得到：
<img src="pictures/3-0-embedding.png" alt="Bert-embedding"></img> 图：Bert-embedding</p>
<ol>
<li>word_embeddings，上文中 subword 对应的嵌入。</li>
<li>token_type_embeddings，用于表示当前词所在的句子，辅助区别句子与 padding、句子对间的差异。
3。 position_embeddings，句子中每个词的位置嵌入，用于区别词的顺序。和 transformer 论文中的设计不同，这一块是训练出来的，而不是通过 Sinusoidal 函数计算得到的固定嵌入。一般认为这种实现不利于拓展性（难以直接迁移到更长的句子中）。</li>
</ol>
<p>三个 embedding 不带权重相加，并通过一层 LayerNorm+dropout 后输出，其大小为(batch_size, sequence_length, hidden_size)。</p>
<p><strong> <a href="https://www.zhihu.com/question/395811291/answer/1260290120" target="_blank">这里为什么要用 LayerNorm+Dropout 呢？为什么要用 LayerNorm 而不是 BatchNorm？可以参考一个不错的回答：transformer 为什么使用 layer normalization，而不是其他的归一化方法？</a></strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertEmbeddings</span>(nn.Module):
    <span class="hljs-string">"""Construct the embeddings from word, position and token_type embeddings."""</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        <span class="hljs-variable language_">self</span>.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        <span class="hljs-variable language_">self</span>.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)

        <span class="hljs-comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span>
        <span class="hljs-comment"># any TensorFlow checkpoint file</span>
        <span class="hljs-variable language_">self</span>.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(config.hidden_dropout_prob)
        <span class="hljs-comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span>
        <span class="hljs-variable language_">self</span>.position_embedding_type = <span class="hljs-built_in">getattr</span>(config, <span class="hljs-string">"position_embedding_type"</span>, <span class="hljs-string">"absolute"</span>)
        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">"position_ids"</span>, torch.arange(config.max_position_embeddings).expand((<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)))
        <span class="hljs-keyword">if</span> version.parse(torch.__version__) &gt; version.parse(<span class="hljs-string">"1.6.0"</span>):
            <span class="hljs-variable language_">self</span>.register_buffer(
                <span class="hljs-string">"token_type_ids"</span>,
                torch.zeros(<span class="hljs-variable language_">self</span>.position_ids.size(), dtype=torch.long, device=<span class="hljs-variable language_">self</span>.position_ids.device),
                persistent=<span class="hljs-literal">False</span>,
            )

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">
        self, input_ids=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, position_ids=<span class="hljs-literal">None</span>, inputs_embeds=<span class="hljs-literal">None</span>, past_key_values_length=<span class="hljs-number">0</span>
    </span>):
        <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            input_shape = input_ids.size()
        <span class="hljs-keyword">else</span>:
            input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]

        seq_length = input_shape[<span class="hljs-number">1</span>]

        <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            position_ids = <span class="hljs-variable language_">self</span>.position_ids[:, past_key_values_length : seq_length + past_key_values_length]

        <span class="hljs-comment"># Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs</span>
        <span class="hljs-comment"># when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves</span>
        <span class="hljs-comment"># issue #5664</span>
        <span class="hljs-keyword">if</span> token_type_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(<span class="hljs-variable language_">self</span>, <span class="hljs-string">"token_type_ids"</span>):
                buffered_token_type_ids = <span class="hljs-variable language_">self</span>.token_type_ids[:, :seq_length]
                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[<span class="hljs-number">0</span>], seq_length)
                token_type_ids = buffered_token_type_ids_expanded
            <span class="hljs-keyword">else</span>:
                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=<span class="hljs-variable language_">self</span>.position_ids.device)

        <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            inputs_embeds = <span class="hljs-variable language_">self</span>.word_embeddings(input_ids)
        token_type_embeddings = <span class="hljs-variable language_">self</span>.token_type_embeddings(token_type_ids)

        embeddings = inputs_embeds + token_type_embeddings
        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">"absolute"</span>:
            position_embeddings = <span class="hljs-variable language_">self</span>.position_embeddings(position_ids)
            embeddings += position_embeddings
        embeddings = <span class="hljs-variable language_">self</span>.LayerNorm(embeddings)
        embeddings = <span class="hljs-variable language_">self</span>.dropout(embeddings)
        <span class="hljs-keyword">return</span> embeddings
</code></pre>
<hr></hr>
<h3 id="22-bertencoder">2.2-BertEncoder</h3>
<p>包含多层 BertLayer，这一块本身没有特别需要说明的地方，不过有一个细节值得参考：利用 gradient checkpointing 技术以降低训练时的显存占用。</p>
<p><strong>gradient checkpointing 即梯度检查点，通过减少保存的计算图节点压缩模型占用空间，但是在计算梯度的时候需要重新计算没有存储的值，参考论文《Training Deep Nets with Sublinear Memory Cost》，过程如下示意图</strong>
<img src="pictures/3-1-gradient-checkpointing.gif" alt="gradient-checkpointing"></img> 图：gradient-checkpointing</p>
<p>在 BertEncoder 中，gradient checkpoint 是通过 torch.utils.checkpoint.checkpoint 实现的，使用起来比较方便，可以参考文档：torch.utils.checkpoint - PyTorch 1.8.1 documentation，这一机制的具体实现比较复杂，在此不作展开。</p>
<p>再往深一层走，就进入了 Encoder 的某一层：</p>
<pre><code class="lang-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertEncoder</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.config = config
        <span class="hljs-variable language_">self</span>.layer = nn.ModuleList([BertLayer(config) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.num_hidden_layers)])

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">
        self,
        hidden_states,
        attention_mask=<span class="hljs-literal">None</span>,
        head_mask=<span class="hljs-literal">None</span>,
        encoder_hidden_states=<span class="hljs-literal">None</span>,
        encoder_attention_mask=<span class="hljs-literal">None</span>,
        past_key_values=<span class="hljs-literal">None</span>,
        use_cache=<span class="hljs-literal">None</span>,
        output_attentions=<span class="hljs-literal">False</span>,
        output_hidden_states=<span class="hljs-literal">False</span>,
        return_dict=<span class="hljs-literal">True</span>,
    </span>):
        all_hidden_states = () <span class="hljs-keyword">if</span> output_hidden_states <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>
        all_self_attentions = () <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>
        all_cross_attentions = () <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.config.add_cross_attention <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>

        next_decoder_cache = () <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>
        <span class="hljs-keyword">for</span> i, layer_module <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.layer):
            <span class="hljs-keyword">if</span> output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_head_mask = head_mask[i] <span class="hljs-keyword">if</span> head_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>
            past_key_value = past_key_values[i] <span class="hljs-keyword">if</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>

            <span class="hljs-keyword">if</span> <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>.config, <span class="hljs-string">"gradient_checkpointing"</span>, <span class="hljs-literal">False</span>) <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.training:

                <span class="hljs-keyword">if</span> use_cache:
                    logger.warning(
                        <span class="hljs-string">"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting "</span>
                        <span class="hljs-string">"`use_cache=False`..."</span>
                    )
                    use_cache = <span class="hljs-literal">False</span>

                <span class="hljs-keyword">def</span> <span class="hljs-title function_">create_custom_forward</span>(<span class="hljs-params">module</span>):
                    <span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_forward</span>(<span class="hljs-params">*inputs</span>):
                        <span class="hljs-keyword">return</span> module(*inputs, past_key_value, output_attentions)

                    <span class="hljs-keyword">return</span> custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(layer_module),
                    hidden_states,
                    attention_mask,
                    layer_head_mask,
                    encoder_hidden_states,
                    encoder_attention_mask,
                )
            <span class="hljs-keyword">else</span>:
                layer_outputs = layer_module(
                    hidden_states,
                    attention_mask,
                    layer_head_mask,
                    encoder_hidden_states,
                    encoder_attention_mask,
                    past_key_value,
                    output_attentions,
                )

            hidden_states = layer_outputs[<span class="hljs-number">0</span>]
            <span class="hljs-keyword">if</span> use_cache:
                next_decoder_cache += (layer_outputs[-<span class="hljs-number">1</span>],)
            <span class="hljs-keyword">if</span> output_attentions:
                all_self_attentions = all_self_attentions + (layer_outputs[<span class="hljs-number">1</span>],)
                <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.config.add_cross_attention:
                    all_cross_attentions = all_cross_attentions + (layer_outputs[<span class="hljs-number">2</span>],)

        <span class="hljs-keyword">if</span> output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:
            <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>(
                v
                <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> [
                    hidden_states,
                    next_decoder_cache,
                    all_hidden_states,
                    all_self_attentions,
                    all_cross_attentions,
                ]
                <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
            )
        <span class="hljs-keyword">return</span> BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=hidden_states,
            past_key_values=next_decoder_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
            cross_attentions=all_cross_attentions,
        )
</code></pre>
<hr></hr>
<h4 id="2211-bertattention">2.2.1.1 BertAttention</h4>
<p>本以为 attention 的实现就在这里，没想到还要再下一层……其中，self 成员就是多头注意力的实现，而 output 成员实现 attention 后的全连接 +dropout+residual+LayerNorm 一系列操作。</p>
<pre><code>class BertAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.self = BertSelfAttention(config)
        self.output = BertSelfOutput(config)
        self.pruned_heads = set()
</code></pre><p>首先还是回到这一层。这里出现了上文提到的剪枝操作，即 prune_heads 方法：</p>
<pre><code> def prune_heads(self, heads):
        if len(heads) == 0:
            return
        heads, index = find_pruneable_heads_and_indices(
            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads
        )

        # Prune linear layers
        self.self.query = prune_linear_layer(self.self.query, index)
        self.self.key = prune_linear_layer(self.self.key, index)
        self.self.value = prune_linear_layer(self.self.value, index)
        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)

        # Update hyper params and store pruned heads
        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)
        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads
        self.pruned_heads = self.pruned_heads.union(heads)
</code></pre><p>这里的具体实现概括如下：</p>
<ul>
<li><p><code>find_pruneable_heads_and_indices</code>是定位需要剪掉的 head，以及需要保留的维度下标 index；</p>
</li>
<li><p><code>prune_linear_layer</code>则负责将 Wk/Wq/Wv 权重矩阵（连同 bias）中按照 index 保留没有被剪枝的维度后转移到新的矩阵。
接下来就到重头戏——Self-Attention 的具体实现。</p>
</li>
</ul>
<pre><code class="lang-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertAttention</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span> = BertSelfAttention(config)
        <span class="hljs-variable language_">self</span>.output = BertSelfOutput(config)
        <span class="hljs-variable language_">self</span>.pruned_heads = <span class="hljs-built_in">set</span>()

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">prune_heads</span>(<span class="hljs-params">self, heads</span>):
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(heads) == <span class="hljs-number">0</span>:
            <span class="hljs-keyword">return</span>
        heads, index = find_pruneable_heads_and_indices(
            heads, <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.num_attention_heads, <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.attention_head_size, <span class="hljs-variable language_">self</span>.pruned_heads
        )

        <span class="hljs-comment"># Prune linear layers</span>
        <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.query = prune_linear_layer(<span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.query, index)
        <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.key = prune_linear_layer(<span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.key, index)
        <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.value = prune_linear_layer(<span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.value, index)
        <span class="hljs-variable language_">self</span>.output.dense = prune_linear_layer(<span class="hljs-variable language_">self</span>.output.dense, index, dim=<span class="hljs-number">1</span>)

        <span class="hljs-comment"># Update hyper params and store pruned heads</span>
        <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.num_attention_heads = <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.num_attention_heads - <span class="hljs-built_in">len</span>(heads)
        <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.all_head_size = <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.attention_head_size * <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>.num_attention_heads
        <span class="hljs-variable language_">self</span>.pruned_heads = <span class="hljs-variable language_">self</span>.pruned_heads.union(heads)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">
        self,
        hidden_states,
        attention_mask=<span class="hljs-literal">None</span>,
        head_mask=<span class="hljs-literal">None</span>,
        encoder_hidden_states=<span class="hljs-literal">None</span>,
        encoder_attention_mask=<span class="hljs-literal">None</span>,
        past_key_value=<span class="hljs-literal">None</span>,
        output_attentions=<span class="hljs-literal">False</span>,
    </span>):
        self_outputs = <span class="hljs-variable language_">self</span>.<span class="hljs-variable language_">self</span>(
            hidden_states,
            attention_mask,
            head_mask,
            encoder_hidden_states,
            encoder_attention_mask,
            past_key_value,
            output_attentions,
        )
        attention_output = <span class="hljs-variable language_">self</span>.output(self_outputs[<span class="hljs-number">0</span>], hidden_states)
        outputs = (attention_output,) + self_outputs[<span class="hljs-number">1</span>:]  <span class="hljs-comment"># add attentions if we output them</span>
        <span class="hljs-keyword">return</span> outputs
</code></pre>
<hr></hr>
<h5 id="22111-bertselfattention">2.2.1.1.1 BertSelfAttention</h5>
<p><strong>预警：这一块可以说是模型的核心区域，也是唯一涉及到公式的地方，所以将贴出大量代码。</strong></p>
<p>初始化部分：</p>
<pre><code>class BertSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, "embedding_size"):
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads)
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
        if self.position_embedding_type == "relative_key" or self.position_embedding_type == "relative_key_query":
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)

        self.is_decoder = config.is_decoder
</code></pre><ul>
<li>除掉熟悉的 query、key、value 三个权重和一个 dropout，这里还有一个谜一样的 position_embedding_type，以及 decoder 标记；</li>
<li><p>注意，hidden_size 和 all_head_size 在一开始是一样的。至于为什么要看起来多此一举地设置这一个变量——显然是因为上面那个剪枝函数，剪掉几个 attention head 以后 all_head_size 自然就小了；</p>
</li>
<li><p>hidden_size 必须是 num_attention_heads 的整数倍，以 bert-base 为例，每个 attention 包含 12 个 head，hidden_size 是 768，所以每个 head 大小即 attention_head_size=768/12=64；</p>
</li>
<li><p>position_embedding_type 是什么？继续往下看就知道了.</p>
</li>
</ul>
<p>然后是重点，也就是前向传播过程。</p>
<p>首先回顾一下 multi-head self-attention 的基本公式：</p>
<p>$$MHA(Q, K, V) = Concat(head_1, ..., head_h)W^O$$</p>
<p>$$head_i = SDPA(QW_i^Q, KW_i^K, VW_i^V)$$</p>
<p>$$SDPA(Q, K, V) = softmax(\frac{QK^T}{\sqrt(d_k)})V$$</p>
<p>而这些注意力头，众所周知是并行计算的，所以上面的 query、key、value 三个权重是唯一的——这并不是所有 heads 共享了权重，而是“拼接”起来了。</p>
<p><strong><a href="https://www.zhihu.com/question/341222779/answer/814111138" target="_blank">原论文中多头的理由为 Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. 而另一个比较靠谱的分析有：为什么 Transformer 需要进行 Multi-head Attention？</a></strong></p>
<p>看看 forward 方法：</p>
<pre><code>def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
    ):
        mixed_query_layer = self.query(hidden_states)

        # 省略一部分cross-attention的计算
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        value_layer = self.transpose_for_scores(self.value(hidden_states))
        query_layer = self.transpose_for_scores(mixed_query_layer)

        # Take the dot product between "query" and "key" to get the raw attention scores.
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        # ...
</code></pre><p>这里的 <code>transpose_for_scores</code> 用来把 <code>hidden_size</code> 拆成多个头输出的形状，并且将中间两维转置以进行矩阵相乘；</p>
<p>这里 <code>key_layer/value_layer/query_laye</code>r 的形状为：(batch_size, num_attention_heads, sequence_length, attention_head_size)；
这里 <code>attention_scores</code> 的形状为：(batch_size, num_attention_heads, sequence_length, sequence_length)，符合多个头单独计算获得的 attention map 形状。</p>
<p>到这里实现了 K 与 Q 相乘，获得 raw attention scores 的部分，按公式接下来应该是按 $d_k$ 进行 scaling 并做 softmax 的操作。然而先出现在眼前的是一个奇怪的positional_embedding，以及一堆爱因斯坦求和：</p>
<pre><code> # ...
        if self.position_embedding_type == "relative_key" or self.position_embedding_type == "relative_key_query":
            seq_length = hidden_states.size()[1]
            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
            distance = position_ids_l - position_ids_r
            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)
            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility

            if self.position_embedding_type == "relative_key":
                relative_position_scores = torch.einsum("bhld,lrd-&gt;bhlr", query_layer, positional_embedding)
                attention_scores = attention_scores + relative_position_scores
            elif self.position_embedding_type == "relative_key_query":
                relative_position_scores_query = torch.einsum("bhld,lrd-&gt;bhlr", query_layer, positional_embedding)
                relative_position_scores_key = torch.einsum("bhrd,lrd-&gt;bhlr", key_layer, positional_embedding)
                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key
        # ...
</code></pre><p><strong><a href="https://pytorch.org/docs/stable/generated/torch.einsum.html" target="_blank">关于爱因斯坦求和约定，参考以下文档：torch.einsum - PyTorch 1.8.1 documentation</a></strong></p>
<p>对于不同的positional_embedding_type，有三种操作：</p>
<ul>
<li>absolute：默认值，这部分就不用处理；</li>
<li>relative_key：对 key_layer 作处理，将其与这里的positional_embedding和 key 矩阵相乘作为 key 相关的位置编码；</li>
<li>relative_key_query：对 key 和 value 都进行相乘以作为位置编码。</li>
</ul>
<p>回到正常 attention 的流程：</p>
<pre><code># ...
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        if attention_mask is not None:
            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
            attention_scores = attention_scores + attention_mask  # 这里为什么是+而不是*？

        # Normalize the attention scores to probabilities.
        attention_probs = nn.Softmax(dim=-1)(attention_scores)

        # This is actually dropping out entire tokens to attend to, which might
        # seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs = self.dropout(attention_probs)

        # Mask heads if we want to
        if head_mask is not None:
            attention_probs = attention_probs * head_mask

        context_layer = torch.matmul(attention_probs, value_layer)

        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)

        # 省略decoder返回值部分……
        return outputs
</code></pre><p>重大疑问：这里的attention_scores = attention_scores + attention_mask是在做什么？难道不应该是乘 mask 吗？</p>
<ul>
<li>因为这里的 attention_mask 已经【被动过手脚】，将原本为 1 的部分变为 0，而原本为 0 的部分（即 padding）变为一个较大的负数，这样相加就得到了一个较大的负值：</li>
<li>至于为什么要用【一个较大的负数】？因为这样一来经过 softmax 操作以后这一项就会变成接近 0 的小数。</li>
</ul>
<pre><code>(Pdb) attention_mask
tensor([[[[    -0.,     -0.,     -0.,  ..., -10000., -10000., -10000.]]],
        [[[    -0.,     -0.,     -0.,  ..., -10000., -10000., -10000.]]],
        [[[    -0.,     -0.,     -0.,  ..., -10000., -10000., -10000.]]],
        ...,
        [[[    -0.,     -0.,     -0.,  ..., -10000., -10000., -10000.]]],
        [[[    -0.,     -0.,     -0.,  ..., -10000., -10000., -10000.]]],
        [[[    -0.,     -0.,     -0.,  ..., -10000., -10000., -10000.]]]],
       device='cuda:0')
</code></pre><p>那么，这一步是在哪里执行的呢？
在modeling_bert.py中没有找到答案，但是在modeling_utils.py中找到了一个特别的类：class ModuleUtilsMixin，在它的get_extended_attention_mask方法中发现了端倪：</p>
<pre><code> def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device) -&gt; Tensor:
        """
        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.

        Arguments:
            attention_mask (:obj:`torch.Tensor`):
                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.
            input_shape (:obj:`Tuple[int]`):
                The shape of the input to the model.
            device: (:obj:`torch.device`):
                The device of the input to the model.

        Returns:
            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.
        """
        # 省略一部分……

        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
        # masked positions, this operation will create a tensor which is 0.0 for
        # positions we want to attend and -10000.0 for masked positions.
        # Since we are adding it to the raw scores before the softmax, this is
        # effectively the same as removing these entirely.
        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        return extended_attention_mask
</code></pre><p>那么，这个函数是在什么时候被调用的呢？和BertModel有什么关系呢？
OK，这里涉及到 <code>BertModel</code> 的继承细节了：<code>BertModel</code>继承自<code>BertPreTrainedModel</code>，后者继承自<code>PreTrainedModel</code>，而<code>PreTrainedModel</code>继承自[nn.Module, ModuleUtilsMixin, GenerationMixin]三个基类。——好复杂的封装！</p>
<p>这也就是说，BertModel必然在中间的某个步骤对原始的attention_mask调用了get_extended_attention_mask，导致attention_mask从原始的[1, 0]变为[0, -1e4]的取值。</p>
<p>最终在 BertModel 的前向传播过程中找到了这一调用（第 944 行）：</p>
<pre><code>  # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]
        # ourselves in which case we just need to make it broadcastable to all heads.
        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)
</code></pre><p>问题解决了：这一方法不但实现了改变 mask 的值，还将其广播（broadcast）为可以直接与 attention map 相加的形状。
不愧是你，HuggingFace。</p>
<p>除此之外，值得注意的细节有：</p>
<ul>
<li>按照每个头的维度进行缩放，对于 bert-base 就是 64 的平方根即 8；</li>
<li>attention_probs 不但做了 softmax，还用了一次 dropout，这是担心 attention 矩阵太稠密吗…… 这里也提到很不寻常，但是原始 Transformer 论文就是这么做的；</li>
<li>head_mask 就是之前提到的对多头计算的 mask，如果不设置默认是全 1，在这里就不会起作用；</li>
<li>context_layer 即 attention 矩阵与 value 矩阵的乘积，原始的大小为：(batch_size, num_attention_heads, sequence_length, attention_head_size) ；</li>
<li>context_layer 进行转置和 view 操作以后，形状就恢复了(batch_size, sequence_length, hidden_size)。</li>
</ul>
<pre><code class="lang-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertSelfAttention</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(config, <span class="hljs-string">"embedding_size"</span>):
            <span class="hljs-keyword">raise</span> ValueError(
                <span class="hljs-string">f"The hidden size (<span class="hljs-subst">{config.hidden_size}</span>) is not a multiple of the number of attention "</span>
                <span class="hljs-string">f"heads (<span class="hljs-subst">{config.num_attention_heads}</span>)"</span>
            )

        <span class="hljs-variable language_">self</span>.num_attention_heads = config.num_attention_heads
        <span class="hljs-variable language_">self</span>.attention_head_size = <span class="hljs-built_in">int</span>(config.hidden_size / config.num_attention_heads)
        <span class="hljs-variable language_">self</span>.all_head_size = <span class="hljs-variable language_">self</span>.num_attention_heads * <span class="hljs-variable language_">self</span>.attention_head_size

        <span class="hljs-variable language_">self</span>.query = nn.Linear(config.hidden_size, <span class="hljs-variable language_">self</span>.all_head_size)
        <span class="hljs-variable language_">self</span>.key = nn.Linear(config.hidden_size, <span class="hljs-variable language_">self</span>.all_head_size)
        <span class="hljs-variable language_">self</span>.value = nn.Linear(config.hidden_size, <span class="hljs-variable language_">self</span>.all_head_size)

        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        <span class="hljs-variable language_">self</span>.position_embedding_type = <span class="hljs-built_in">getattr</span>(config, <span class="hljs-string">"position_embedding_type"</span>, <span class="hljs-string">"absolute"</span>)
        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">"relative_key"</span> <span class="hljs-keyword">or</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">"relative_key_query"</span>:
            <span class="hljs-variable language_">self</span>.max_position_embeddings = config.max_position_embeddings
            <span class="hljs-variable language_">self</span>.distance_embedding = nn.Embedding(<span class="hljs-number">2</span> * config.max_position_embeddings - <span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.attention_head_size)

        <span class="hljs-variable language_">self</span>.is_decoder = config.is_decoder

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">transpose_for_scores</span>(<span class="hljs-params">self, x</span>):
        new_x_shape = x.size()[:-<span class="hljs-number">1</span>] + (<span class="hljs-variable language_">self</span>.num_attention_heads, <span class="hljs-variable language_">self</span>.attention_head_size)
        x = x.view(*new_x_shape)
        <span class="hljs-keyword">return</span> x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">
        self,
        hidden_states,
        attention_mask=<span class="hljs-literal">None</span>,
        head_mask=<span class="hljs-literal">None</span>,
        encoder_hidden_states=<span class="hljs-literal">None</span>,
        encoder_attention_mask=<span class="hljs-literal">None</span>,
        past_key_value=<span class="hljs-literal">None</span>,
        output_attentions=<span class="hljs-literal">False</span>,
    </span>):
        mixed_query_layer = <span class="hljs-variable language_">self</span>.query(hidden_states)

        <span class="hljs-comment"># If this is instantiated as a cross-attention module, the keys</span>
        <span class="hljs-comment"># and values come from an encoder; the attention mask needs to be</span>
        <span class="hljs-comment"># such that the encoder's padding tokens are not attended to.</span>
        is_cross_attention = encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>

        <span class="hljs-keyword">if</span> is_cross_attention <span class="hljs-keyword">and</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            <span class="hljs-comment"># reuse k,v, cross_attentions</span>
            key_layer = past_key_value[<span class="hljs-number">0</span>]
            value_layer = past_key_value[<span class="hljs-number">1</span>]
            attention_mask = encoder_attention_mask
        <span class="hljs-keyword">elif</span> is_cross_attention:
            key_layer = <span class="hljs-variable language_">self</span>.transpose_for_scores(<span class="hljs-variable language_">self</span>.key(encoder_hidden_states))
            value_layer = <span class="hljs-variable language_">self</span>.transpose_for_scores(<span class="hljs-variable language_">self</span>.value(encoder_hidden_states))
            attention_mask = encoder_attention_mask
        <span class="hljs-keyword">elif</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            key_layer = <span class="hljs-variable language_">self</span>.transpose_for_scores(<span class="hljs-variable language_">self</span>.key(hidden_states))
            value_layer = <span class="hljs-variable language_">self</span>.transpose_for_scores(<span class="hljs-variable language_">self</span>.value(hidden_states))
            key_layer = torch.cat([past_key_value[<span class="hljs-number">0</span>], key_layer], dim=<span class="hljs-number">2</span>)
            value_layer = torch.cat([past_key_value[<span class="hljs-number">1</span>], value_layer], dim=<span class="hljs-number">2</span>)
        <span class="hljs-keyword">else</span>:
            key_layer = <span class="hljs-variable language_">self</span>.transpose_for_scores(<span class="hljs-variable language_">self</span>.key(hidden_states))
            value_layer = <span class="hljs-variable language_">self</span>.transpose_for_scores(<span class="hljs-variable language_">self</span>.value(hidden_states))

        query_layer = <span class="hljs-variable language_">self</span>.transpose_for_scores(mixed_query_layer)

        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.is_decoder:
            <span class="hljs-comment"># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span>
            <span class="hljs-comment"># Further calls to cross_attention layer can then reuse all cross-attention</span>
            <span class="hljs-comment"># key/value_states (first "if" case)</span>
            <span class="hljs-comment"># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span>
            <span class="hljs-comment"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span>
            <span class="hljs-comment"># can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)</span>
            <span class="hljs-comment"># if encoder bi-directional self-attention `past_key_value` is always `None`</span>
            past_key_value = (key_layer, value_layer)

        <span class="hljs-comment"># Take the dot product between "query" and "key" to get the raw attention scores.</span>
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>))

        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">"relative_key"</span> <span class="hljs-keyword">or</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">"relative_key_query"</span>:
            seq_length = hidden_states.size()[<span class="hljs-number">1</span>]
            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)
            distance = position_ids_l - position_ids_r
            positional_embedding = <span class="hljs-variable language_">self</span>.distance_embedding(distance + <span class="hljs-variable language_">self</span>.max_position_embeddings - <span class="hljs-number">1</span>)
            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  <span class="hljs-comment"># fp16 compatibility</span>

            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">"relative_key"</span>:
                relative_position_scores = torch.einsum(<span class="hljs-string">"bhld,lrd-&gt;bhlr"</span>, query_layer, positional_embedding)
                attention_scores = attention_scores + relative_position_scores
            <span class="hljs-keyword">elif</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">"relative_key_query"</span>:
                relative_position_scores_query = torch.einsum(<span class="hljs-string">"bhld,lrd-&gt;bhlr"</span>, query_layer, positional_embedding)
                relative_position_scores_key = torch.einsum(<span class="hljs-string">"bhrd,lrd-&gt;bhlr"</span>, key_layer, positional_embedding)
                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key

        attention_scores = attention_scores / math.sqrt(<span class="hljs-variable language_">self</span>.attention_head_size)
        <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            <span class="hljs-comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span>
            attention_scores = attention_scores + attention_mask

        <span class="hljs-comment"># Normalize the attention scores to probabilities.</span>
        attention_probs = nn.Softmax(dim=-<span class="hljs-number">1</span>)(attention_scores)

        <span class="hljs-comment"># This is actually dropping out entire tokens to attend to, which might</span>
        <span class="hljs-comment"># seem a bit unusual, but is taken from the original Transformer paper.</span>
        attention_probs = <span class="hljs-variable language_">self</span>.dropout(attention_probs)

        <span class="hljs-comment"># Mask heads if we want to</span>
        <span class="hljs-keyword">if</span> head_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            attention_probs = attention_probs * head_mask

        context_layer = torch.matmul(attention_probs, value_layer)

        context_layer = context_layer.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous()
        new_context_layer_shape = context_layer.size()[:-<span class="hljs-number">2</span>] + (<span class="hljs-variable language_">self</span>.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        outputs = (context_layer, attention_probs) <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">else</span> (context_layer,)

        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.is_decoder:
            outputs = outputs + (past_key_value,)
        <span class="hljs-keyword">return</span> outputs
</code></pre>
<hr></hr>
<h5 id="22112-bertselfoutput">2.2.1.1.2 BertSelfOutput</h5>
<pre><code>class BertSelfOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states
</code></pre><p><strong>这里又出现了 LayerNorm 和 Dropout 的组合，只不过这里是先 Dropout，进行残差连接后再进行 LayerNorm。至于为什么要做残差连接，最直接的目的就是降低网络层数过深带来的训练难度，对原始输入更加敏感～</strong></p>
<pre><code class="lang-python">
<span class="hljs-keyword">class</span> <span class="hljs-title class_">BertSelfOutput</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.dense = nn.Linear(config.hidden_size, config.hidden_size)
        <span class="hljs-variable language_">self</span>.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(config.hidden_dropout_prob)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states, input_tensor</span>):
        hidden_states = <span class="hljs-variable language_">self</span>.dense(hidden_states)
        hidden_states = <span class="hljs-variable language_">self</span>.dropout(hidden_states)
        hidden_states = <span class="hljs-variable language_">self</span>.LayerNorm(hidden_states + input_tensor)
        <span class="hljs-keyword">return</span> hidden_states
</code></pre>
<hr></hr>
<h4 id="2212-bertintermediate">2.2.1.2 BertIntermediate</h4>
<p>看完了 BertAttention，在 Attention 后面还有一个全连接+激活的操作：</p>
<pre><code>class BertIntermediate(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states
</code></pre><ul>
<li>这里的全连接做了一个扩展，以 bert-base 为例，扩展维度为 3072，是原始维度 768 的 4 倍之多；</li>
<li>这里的激活函数默认实现为 gelu（Gaussian Error Linerar Units(GELUS）当然，它是无法直接计算的，可以用一个包含tanh的表达式进行近似（略)。</li>
</ul>
<pre><code class="lang-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertIntermediate</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(config.hidden_act, <span class="hljs-built_in">str</span>):
            <span class="hljs-variable language_">self</span>.intermediate_act_fn = ACT2FN[config.hidden_act]
        <span class="hljs-keyword">else</span>:
            <span class="hljs-variable language_">self</span>.intermediate_act_fn = config.hidden_act

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states</span>):
        hidden_states = <span class="hljs-variable language_">self</span>.dense(hidden_states)
        hidden_states = <span class="hljs-variable language_">self</span>.intermediate_act_fn(hidden_states)
        <span class="hljs-keyword">return</span> hidden_states
</code></pre>
<hr></hr>
<h4 id="2213-bertoutput">2.2.1.3 BertOutput</h4>
<p>在这里又是一个全连接 +dropout+LayerNorm，还有一个残差连接 residual connect：</p>
<pre><code>class BertOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states
</code></pre><p>这里的操作和 BertSelfOutput 不能说没有关系，只能说一模一样…… 非常容易混淆的两个组件。
以下内容还包含基于 BERT 的应用模型，以及 BERT 相关的优化器和用法，将在下一篇文章作详细介绍。</p>
<pre><code class="lang-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertOutput</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        <span class="hljs-variable language_">self</span>.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(config.hidden_dropout_prob)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states, input_tensor</span>):
        hidden_states = <span class="hljs-variable language_">self</span>.dense(hidden_states)
        hidden_states = <span class="hljs-variable language_">self</span>.dropout(hidden_states)
        hidden_states = <span class="hljs-variable language_">self</span>.LayerNorm(hidden_states + input_tensor)
        <span class="hljs-keyword">return</span> hidden_states
</code></pre>
<hr></hr>
<h3 id="223-bertpooler">2.2.3 BertPooler</h3>
<p>这一层只是简单地取出了句子的第一个token，即<code>[CLS]</code>对应的向量，然后过一个全连接层和一个激活函数后输出：（这一部分是可选的，因为pooling有很多不同的操作）</p>
<pre><code>class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
</code></pre><pre><code class="lang-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertPooler</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.dense = nn.Linear(config.hidden_size, config.hidden_size)
        <span class="hljs-variable language_">self</span>.activation = nn.Tanh()

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states</span>):
        <span class="hljs-comment"># We "pool" the model by simply taking the hidden state corresponding</span>
        <span class="hljs-comment"># to the first token.</span>
        first_token_tensor = hidden_states[:, <span class="hljs-number">0</span>]
        pooled_output = <span class="hljs-variable language_">self</span>.dense(first_token_tensor)
        pooled_output = <span class="hljs-variable language_">self</span>.activation(pooled_output)
        <span class="hljs-keyword">return</span> pooled_output
<span class="hljs-keyword">from</span> transformers.models.bert.configuration_bert <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">import</span> torch
config = BertConfig.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>)
bert_pooler = BertPooler(config=config)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"input to bert pooler size: {}"</span>.<span class="hljs-built_in">format</span>(config.hidden_size))
batch_size = <span class="hljs-number">1</span>
seq_len = <span class="hljs-number">2</span>
hidden_size = <span class="hljs-number">768</span>
x = torch.rand(batch_size, seq_len, hidden_size)
y = bert_pooler(x)
<span class="hljs-built_in">print</span>(y.size())
</code></pre>
<pre><code>input to bert pooler size: 768
torch.Size([1, 768])
</code></pre><pre><code class="lang-python">

</code></pre>
<h2 id="小总结">小总结</h2>
<p>本小节对Bert模型的实现进行分析了学习，希望读者能对Bert实现有一个更为细致的把握。</p>
<p>值得注意的是，在 HuggingFace 实现的 Bert 模型中，使用了多种节约显存的技术：</p>
<ul>
<li>gradient checkpoint，不保留前向传播节点，只在用时计算；apply_chunking_to_forward，按多个小批量和低维度计算 FFN 部</li>
<li>BertModel 包含复杂的封装和较多的组件。以 bert-base 为例，主要组件如下：<ul>
<li>总计Dropout出现了1+(1+1+1)x12=37次；</li>
<li>总计LayerNorm出现了1+(1+1)x12=25次；
BertModel 有极大的参数量。以 bert-base 为例，其参数量为 109M。</li>
</ul>
</li>
</ul>
<h2 id="致谢">致谢</h2>
<p>本文主要由浙江大学李泺秋撰写，本项目同学负责整理和汇总。</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../篇章2-Transformer相关原理/2.5-篇章小测.html" class="navigation navigation-prev " aria-label="Previous page: 2.5-篇章小测">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="3.2-如何应用一个BERT.html" class="navigation navigation-next " aria-label="Next page: 3.2-如何应用一个BERT">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"3.1-如何实现一个BERT","level":"3.1","depth":1,"next":{"title":"3.2-如何应用一个BERT","level":"3.2","depth":1,"path":"篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT.md","ref":"./篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT.md","articles":[]},"previous":{"title":"2.5-篇章小测","level":"2.8","depth":1,"path":"篇章2-Transformer相关原理/2.5-篇章小测.md","ref":"./篇章2-Transformer相关原理/2.5-篇章小测.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"篇章3-编写一个Transformer模型：BERT/3.1-如何实现一个BERT.md","mtime":"2024-08-23T15:34:37.343Z","type":"markdown"},"gitbook":{"version":"5.1.4","time":"2024-08-23T15:50:04.957Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

